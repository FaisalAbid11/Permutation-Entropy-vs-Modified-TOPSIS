{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbdEhC4cRJXf",
        "outputId": "8828de82-99c9-45bd-b2ec-1a6b50e2a3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVM Metrics:\n",
            "Accuracy: 0.8484848484848485\n",
            "Precision: 0.8626262626262626\n",
            "Recall: 0.8484848484848485\n",
            "F1 Score: 0.8484848484848485\n",
            "MCC: 0.7111111111111111\n",
            "AUC-ROC: 0.8999999999999999\n",
            "G-Mean: 0.8520128672302584\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85        15\n",
            "           1       0.93      0.78      0.85        18\n",
            "\n",
            "    accuracy                           0.85        33\n",
            "   macro avg       0.86      0.86      0.85        33\n",
            "weighted avg       0.86      0.85      0.85        33\n",
            "\n",
            "\n",
            "KNN Metrics:\n",
            "Accuracy: 0.8787878787878788\n",
            "Precision: 0.8856951871657753\n",
            "Recall: 0.8787878787878788\n",
            "F1 Score: 0.8790106951871658\n",
            "MCC: 0.7638428387783323\n",
            "AUC-ROC: 0.9333333333333333\n",
            "G-Mean: 0.8819171036881969\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.88        15\n",
            "           1       0.94      0.83      0.88        18\n",
            "\n",
            "    accuracy                           0.88        33\n",
            "   macro avg       0.88      0.88      0.88        33\n",
            "weighted avg       0.89      0.88      0.88        33\n",
            "\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9393939393939394\n",
            "Precision: 0.9393939393939394\n",
            "Recall: 0.9393939393939394\n",
            "F1 Score: 0.9393939393939394\n",
            "MCC: 0.8777777777777778\n",
            "AUC-ROC: 0.9555555555555555\n",
            "G-Mean: 0.938872452190116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        15\n",
            "           1       0.94      0.94      0.94        18\n",
            "\n",
            "    accuracy                           0.94        33\n",
            "   macro avg       0.94      0.94      0.94        33\n",
            "weighted avg       0.94      0.94      0.94        33\n",
            "\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.911096256684492\n",
            "Recall: 0.9090909090909091\n",
            "F1 Score: 0.9092584834520318\n",
            "MCC: 0.8191937691245882\n",
            "AUC-ROC: 0.9074074074074074\n",
            "G-Mean: 0.9108400680852976\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90        15\n",
            "           1       0.94      0.89      0.91        18\n",
            "\n",
            "    accuracy                           0.91        33\n",
            "   macro avg       0.91      0.91      0.91        33\n",
            "weighted avg       0.91      0.91      0.91        33\n",
            "\n",
            "\n",
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.85\n",
            "Precision: 0.8855263157894737\n",
            "Recall: 0.85\n",
            "F1 Score: 0.8473387778090679\n",
            "MCC: 0.7359320113278901\n",
            "AUC-ROC: 0.8804226918798665\n",
            "G-Mean: 0.842423539174232\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.87        29\n",
            "           1       1.00      0.71      0.83        31\n",
            "\n",
            "    accuracy                           0.85        60\n",
            "   macro avg       0.88      0.85      0.85        60\n",
            "weighted avg       0.89      0.85      0.85        60\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "1.First we imported all the necessary libraries and packages.\n",
        "2.Then we uploaded the datasets (Distressed).\n",
        "3.Then we balanced the dataset using SMOTE.\n",
        "4.Then We splitted the dataset in 70:30 (train:test) ratio\n",
        "5.Then we normalized the data.\n",
        "6.Then we defined and tuned our classifier model, used parameters suitbale for goof performances.\n",
        "7.Then we found optimized threshhold for each model.\n",
        "8.Then we evaluated the models and printed the results\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "file_path=\"https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-TOPSIS-EVALUATION/Distressed_20.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "data.drop_duplicates(inplace=True)\n",
        "X = data.drop('TOI', axis=1)\n",
        "y = data['TOI']\n",
        "\n",
        "\n",
        "smote = SMOTE(sampling_strategy=0.7, random_state=42,k_neighbors=1)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "\n",
        "data_balanced = pd.DataFrame(X, columns=X.columns)\n",
        "data_balanced['TOI'] = y\n",
        "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm = SVC(C=1,probability=True,kernel='linear',class_weight=\"balanced\",random_state=1)\n",
        "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n",
        "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42,max_depth=10,max_features='sqrt',min_samples_split=5)\n",
        "xgb = XGBClassifier(n_estimators=5000,eval_metric='logloss', random_state=42,max_depth=10,colsample_bytree=1,subsample=0.3,gamma=0.3, scale_pos_weight=0.6)\n",
        "\n",
        "\n",
        "models = {'SVM': svm, 'KNN': knn,'Random Forest': rf, 'XGBoost': xgb}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_probs = {name: model.predict_proba(X_test)[:, 1] for name, model in models.items()}\n",
        "\n",
        "\n",
        "def optimize_threshold(y_true, y_prob):\n",
        "    best_mcc, best_thresh = -1, 0.5\n",
        "    for thresh in np.linspace(0.6,0.9, 100):\n",
        "        y_pred_adjusted = (y_prob >= thresh).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_adjusted)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thresh = mcc, thresh\n",
        "    return best_thresh\n",
        "\n",
        "\n",
        "thresholds = {name: optimize_threshold(y_test, y_pred_probs[name]) for name in models}\n",
        "y_preds = {name: (y_pred_probs[name] >= thresholds[name]).astype(int) for name in models}\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"MCC:\", matthews_corrcoef(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"G-Mean:\", geometric_mean_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "for name in models:\n",
        "    evaluate_model(name, y_test, y_preds[name], y_pred_probs[name])\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-TOPSIS-EVALUATION/Distressed_20.csv\")\n",
        "X = data.drop('TOI', axis=1)\n",
        "y = data['TOI']\n",
        "\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42,k_neighbors=1)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "data_balanced = pd.DataFrame(X, columns=X.columns)\n",
        "data_balanced['TOI'] = y\n",
        "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000,solver='liblinear',penalty='l1',C=0.05, class_weight={0:1,1:1})\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr_prob = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "def optimize_threshold(y_true, y_prob):\n",
        "    best_mcc, best_thresh = -1, 0.5\n",
        "    for thresh in np.linspace(0.1, 0.9, 200):\n",
        "        y_pred_adjusted = (y_prob >= thresh).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_adjusted)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thresh = mcc, thresh\n",
        "    return best_thresh\n",
        "\n",
        "\n",
        "lr_thresh = optimize_threshold(y_test, y_pred_lr_prob)\n",
        "\n",
        "\n",
        "\n",
        "y_pred_lr = (y_pred_lr_prob >= lr_thresh).astype(int)\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"MCC:\", matthews_corrcoef(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"G-Mean:\", geometric_mean_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_lr_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.First we imported all the necessary libraries and packages.\n",
        "2.Then we uploaded the datasets (Behavioral).\n",
        "3.Then we balanced the dataset using SMOTE.\n",
        "4.Then We splitted the dataset in 70:30 (train:test) ratio\n",
        "5.Then we normalized the data.\n",
        "6.Then we defined and tuned our classifier model, used parameters suitbale for goof performances.\n",
        "7.Then we found optimized threshhold for each model.\n",
        "8.Then we evaluated the models and printed the results\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE,BorderlineSMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-TOPSIS-EVALUATION/behavioral_30.csv\")\n",
        "X = data.drop('TOI', axis=1)\n",
        "y = data['TOI']\n",
        "\n",
        "smote = BorderlineSMOTE(sampling_strategy=0.8, random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "data_balanced = pd.DataFrame(X, columns=X.columns)\n",
        "data_balanced['TOI'] = y\n",
        "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm = SVC(C=1, kernel='linear', probability=True, class_weight='balanced',random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n",
        "rf = RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42,max_depth=15,max_features='sqrt')\n",
        "xgb = XGBClassifier(n_estimators=100, eval_metric='logloss',scale_pos_weight=2, random_state=42,max_depth=15, colsample_bytree=1,subsample=0.2,gamma=1)\n",
        "\n",
        "\n",
        "models = {'SVM': svm, 'KNN': knn,'Random Forest': rf, 'XGBoost': xgb}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_probs = {name: model.predict_proba(X_test)[:, 1] for name, model in models.items()}\n",
        "\n",
        "def optimize_threshold(y_true, y_prob):\n",
        "    best_mcc, best_thresh = -1, 0.5\n",
        "    for thresh in np.linspace(0.1, 0.9, 200):\n",
        "        y_pred_adjusted = (y_prob >= thresh).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_adjusted)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thresh = mcc, thresh\n",
        "    return best_thresh\n",
        "\n",
        "thresholds = {name: optimize_threshold(y_test, y_pred_probs[name]) for name in models}\n",
        "y_preds = {name: (y_pred_probs[name] >= thresholds[name]).astype(int) for name in models}\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"MCC:\", matthews_corrcoef(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"G-Mean:\", geometric_mean_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "for name in models:\n",
        "    evaluate_model(name, y_test, y_preds[name], y_pred_probs[name])\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-TOPSIS-EVALUATION/behavioral_30.csv\")\n",
        "X = data.drop('TOI', axis=1)\n",
        "y = data['TOI']\n",
        "\n",
        "smote =BorderlineSMOTE(sampling_strategy=1, random_state=42,k_neighbors=1)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, solver='liblinear',C=1,penalty='l1',random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr_prob = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "def optimize_threshold(y_true, y_prob):\n",
        "    best_mcc, best_thresh = -1, 0.5\n",
        "    for thresh in np.linspace(0.1, 0.9, 100):\n",
        "        y_pred_adjusted = (y_prob >= thresh).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_adjusted)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thresh = mcc, thresh\n",
        "    return best_thresh\n",
        "\n",
        "lr_thresh = optimize_threshold(y_test, y_pred_lr_prob)\n",
        "\n",
        "\n",
        "\n",
        "y_pred_lr = (y_pred_lr_prob >= lr_thresh).astype(int)\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"MCC:\", matthews_corrcoef(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"G-Mean:\", geometric_mean_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_lr_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_FFcPdReDo",
        "outputId": "18b94191-edc2-4f20-e0c6-b371949a1022"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVM Metrics:\n",
            "Accuracy: 0.8627450980392157\n",
            "Precision: 0.8638344226579521\n",
            "Recall: 0.8627450980392157\n",
            "F1 Score: 0.8629574847347061\n",
            "MCC: 0.7244616763479587\n",
            "AUC-ROC: 0.8835403726708073\n",
            "G-Mean: 0.8633316946034312\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.87        28\n",
            "           1       0.83      0.87      0.85        23\n",
            "\n",
            "    accuracy                           0.86        51\n",
            "   macro avg       0.86      0.86      0.86        51\n",
            "weighted avg       0.86      0.86      0.86        51\n",
            "\n",
            "\n",
            "KNN Metrics:\n",
            "Accuracy: 0.8431372549019608\n",
            "Precision: 0.8540305010893247\n",
            "Recall: 0.8431372549019608\n",
            "F1 Score: 0.8433785822021116\n",
            "MCC: 0.696597765719191\n",
            "AUC-ROC: 0.9021739130434783\n",
            "G-Mean: 0.8469895538599198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85        28\n",
            "           1       0.78      0.91      0.84        23\n",
            "\n",
            "    accuracy                           0.84        51\n",
            "   macro avg       0.85      0.85      0.84        51\n",
            "weighted avg       0.85      0.84      0.84        51\n",
            "\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.8627450980392157\n",
            "Precision: 0.8947712418300654\n",
            "Recall: 0.8627450980392157\n",
            "F1 Score: 0.8620051794302627\n",
            "MCC: 0.758287544405155\n",
            "AUC-ROC: 0.9184782608695652\n",
            "G-Mean: 0.8660254037844387\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.75      0.86        28\n",
            "           1       0.77      1.00      0.87        23\n",
            "\n",
            "    accuracy                           0.86        51\n",
            "   macro avg       0.88      0.88      0.86        51\n",
            "weighted avg       0.89      0.86      0.86        51\n",
            "\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.8823529411764706\n",
            "Precision: 0.89360929557008\n",
            "Recall: 0.8823529411764706\n",
            "F1 Score: 0.8825339366515838\n",
            "MCC: 0.7755455125006993\n",
            "AUC-ROC: 0.9145962732919254\n",
            "G-Mean: 0.8864052604279183\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.82      0.88        28\n",
            "           1       0.81      0.96      0.88        23\n",
            "\n",
            "    accuracy                           0.88        51\n",
            "   macro avg       0.89      0.89      0.88        51\n",
            "weighted avg       0.89      0.88      0.88        51\n",
            "\n",
            "\n",
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.8214285714285714\n",
            "Precision: 0.8230769230769232\n",
            "Recall: 0.8214285714285714\n",
            "F1 Score: 0.8212005108556832\n",
            "MCC: 0.6445033866354896\n",
            "AUC-ROC: 0.8545918367346939\n",
            "G-Mean: 0.8206518066482897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.81        28\n",
            "           1       0.80      0.86      0.83        28\n",
            "\n",
            "    accuracy                           0.82        56\n",
            "   macro avg       0.82      0.82      0.82        56\n",
            "weighted avg       0.82      0.82      0.82        56\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LozHMKu0VCp1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.First we imported all the necessary libraries and packages.\n",
        "2.Then we uploaded the datasets (Enthusiastic).\n",
        "3.Then we balanced the dataset using SMOTE.\n",
        "4.Then We splitted the dataset in 70:30 (train:test) ratio\n",
        "5.Then we normalized the data.\n",
        "6.Then we defined and tuned our classifier model, used parameters suitbale for goof performances.\n",
        "7.Then we found optimized threshhold for each model.\n",
        "8.Then we evaluated the models and printed the results\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE,BorderlineSMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-TOPSIS-EVALUATION/enthusiastic_50.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('TOI', axis=1)\n",
        "y = data['TOI']\n",
        "\n",
        "\n",
        "smote =SMOTE(sampling_strategy=1,random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "data_balanced = pd.DataFrame(X, columns=X.columns)\n",
        "data_balanced['TOI'] = y\n",
        "data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "lr =  LogisticRegression(C=1, class_weight={0:0.25,1:2},solver='lbfgs', max_iter=1000,random_state=42)\n",
        "svm = SVC(C=0.1, kernel='rbf', probability=True, class_weight='balanced',gamma=1,random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='manhattan')\n",
        "rf = RandomForestClassifier(n_estimators=500, class_weight={0:3,1:1}, random_state=42,max_depth=20,criterion='entropy',min_samples_split=20)\n",
        "xgb = XGBClassifier(eval_metric='logloss',scale_pos_weight=5, random_state=42)\n",
        "\n",
        "models = {'SVM': svm, 'KNN': knn, 'Logistic Regression': lr, 'Random Forest': rf, 'XGBoost': xgb}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_probs = {name: model.predict_proba(X_test)[:, 1] for name, model in models.items()}\n",
        "\n",
        "\n",
        "def optimize_threshold(y_true, y_prob):\n",
        "    best_mcc, best_thresh = -1, 0.5\n",
        "    for thresh in np.arange(0.1, 0.9, 0.005):\n",
        "        y_pred_adjusted = (y_prob >= thresh).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred_adjusted)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thresh = mcc, thresh\n",
        "    return best_thresh\n",
        "\n",
        "\n",
        "thresholds = {name: optimize_threshold(y_test, y_pred_probs[name]) for name in models}\n",
        "y_preds = {name: (y_pred_probs[name] >= thresholds[name]).astype(int) for name in models}\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='binary'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='binary'))\n",
        "    print(\"MCC:\", matthews_corrcoef(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"G-Mean:\", geometric_mean_score(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "for name in models:\n",
        "    evaluate_model(name, y_test, y_preds[name], y_pred_probs[name])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCBAHe6vRp4W",
        "outputId": "6768cc10-6167-4394-c29c-e467a2001d37"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVM Metrics:\n",
            "Accuracy: 0.8778625954198473\n",
            "Precision: 0.8888888888888888\n",
            "Recall: 0.8615384615384616\n",
            "F1 Score: 0.875\n",
            "MCC: 0.7560067164845063\n",
            "AUC-ROC: 0.8833333333333333\n",
            "G-Mean: 0.8775894086434557\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88        66\n",
            "           1       0.89      0.86      0.88        65\n",
            "\n",
            "    accuracy                           0.88       131\n",
            "   macro avg       0.88      0.88      0.88       131\n",
            "weighted avg       0.88      0.88      0.88       131\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[59  7]\n",
            " [ 9 56]]\n",
            "\n",
            "KNN Metrics:\n",
            "Accuracy: 0.8625954198473282\n",
            "Precision: 0.8405797101449275\n",
            "Recall: 0.8923076923076924\n",
            "F1 Score: 0.8656716417910447\n",
            "MCC: 0.726658042293779\n",
            "AUC-ROC: 0.9241258741258741\n",
            "G-Mean: 0.8623164985025763\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        66\n",
            "           1       0.84      0.89      0.87        65\n",
            "\n",
            "    accuracy                           0.86       131\n",
            "   macro avg       0.86      0.86      0.86       131\n",
            "weighted avg       0.86      0.86      0.86       131\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[55 11]\n",
            " [ 7 58]]\n",
            "\n",
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.8320610687022901\n",
            "Precision: 0.7654320987654321\n",
            "Recall: 0.9538461538461539\n",
            "F1 Score: 0.8493150684931506\n",
            "MCC: 0.6854157546217081\n",
            "AUC-ROC: 0.9001165501165501\n",
            "G-Mean: 0.8241687201381034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.71      0.81        66\n",
            "           1       0.77      0.95      0.85        65\n",
            "\n",
            "    accuracy                           0.83       131\n",
            "   macro avg       0.85      0.83      0.83       131\n",
            "weighted avg       0.85      0.83      0.83       131\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[47 19]\n",
            " [ 3 62]]\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.8854961832061069\n",
            "Precision: 0.8676470588235294\n",
            "Recall: 0.9076923076923077\n",
            "F1 Score: 0.8872180451127819\n",
            "MCC: 0.771868628462583\n",
            "AUC-ROC: 0.9189976689976692\n",
            "G-Mean: 0.8853903568009333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88        66\n",
            "           1       0.87      0.91      0.89        65\n",
            "\n",
            "    accuracy                           0.89       131\n",
            "   macro avg       0.89      0.89      0.89       131\n",
            "weighted avg       0.89      0.89      0.89       131\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[57  9]\n",
            " [ 6 59]]\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.8778625954198473\n",
            "Precision: 0.8769230769230769\n",
            "Recall: 0.8769230769230769\n",
            "F1 Score: 0.8769230769230769\n",
            "MCC: 0.7557109557109557\n",
            "AUC-ROC: 0.9206293706293707\n",
            "G-Mean: 0.8778549826875568\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88        66\n",
            "           1       0.88      0.88      0.88        65\n",
            "\n",
            "    accuracy                           0.88       131\n",
            "   macro avg       0.88      0.88      0.88       131\n",
            "weighted avg       0.88      0.88      0.88       131\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[58  8]\n",
            " [ 8 57]]\n"
          ]
        }
      ]
    }
  ]
}