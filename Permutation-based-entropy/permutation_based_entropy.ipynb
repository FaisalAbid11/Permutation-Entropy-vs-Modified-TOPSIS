{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/Permutation-based-entropy/new%20neumerical.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to transform all categorical columns to numerical values\n",
        "def transform_categorical_to_numerical(df):\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply Label Encoding to all categorical columns\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        df[col + '_Encoded'] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Transform the dataset\n",
        "data_transformed = transform_categorical_to_numerical(data)\n",
        "\n",
        "# Save or display the transformed dataset\n",
        "print(data_transformed.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBmgKQbLEkwI",
        "outputId": "d0eec0a8-08dc-429f-8500-b44cb3938831"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Gender  Work tenure  Education  Job position  working hour  \\\n",
            "0    2       1            2          3             1             2   \n",
            "1    2       1            2          3             2             1   \n",
            "2    4       2            3          3             1             3   \n",
            "3    3       1            3          4             2             2   \n",
            "4    3       1            3          4             3             1   \n",
            "\n",
            "   satisfaction with workload  satisfied  compensation   \\\n",
            "0                           4                         3   \n",
            "1                           2                         2   \n",
            "2                           1                         2   \n",
            "3                           2                         2   \n",
            "4                           2                         3   \n",
            "\n",
            "    good relationship with peers    satisfied with career and  opportunity  \\\n",
            "0                               4                                        4   \n",
            "1                               2                                        2   \n",
            "2                               2                                        2   \n",
            "3                               2                                        2   \n",
            "4                               2                                        2   \n",
            "\n",
            "   satisfied with job profession    monthly average expenditure  \\\n",
            "0                               4                             2   \n",
            "1                               2                             4   \n",
            "2                               2                             3   \n",
            "3                               2                             4   \n",
            "4                               3                             3   \n",
            "\n",
            "    satisfied with  work-life balance   work is meaningful   family supports   \\\n",
            "0                                   4                     4                 4   \n",
            "1                                   2                     2                 2   \n",
            "2                                   2                     2                 2   \n",
            "3                                   2                     2                 2   \n",
            "4                                   2                     3                 2   \n",
            "\n",
            "    mentally well and do not have anxiety   TOI (turnover intention)  \n",
            "0                                        4                         0  \n",
            "1                                        2                         1  \n",
            "2                                        2                         1  \n",
            "3                                        2                         0  \n",
            "4                                        2                         1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/Permutation-based-entropy/new%20neumerical.csv')  # Replace with your file path\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = data[\"TOI (turnover intention)\"]\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Display the top features\n",
        "print(importance_scores.head(17))  # Top 5 features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auj8nRi-PbNm",
        "outputId": "b9d1b52e-fc9b-4f92-e9db-5585ea8b0982"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance\n",
            "6               remainder__satisfaction with workload    0.051887\n",
            "9   remainder__ satisfied with career and  opportu...    0.021698\n",
            "7                 remainder__satisfied  compensation     0.019811\n",
            "11            remainder__ monthly average expenditure    0.009434\n",
            "15  remainder__ mentally well and do not have anxi...    0.006604\n",
            "14                        remainder__family supports     0.002830\n",
            "2                              remainder__Work tenure    0.001887\n",
            "8           remainder__ good relationship with peers     0.001887\n",
            "10          remainder__satisfied with job profession     0.000943\n",
            "4                             remainder__Job position    0.000000\n",
            "12      remainder__ satisfied with  work-life balance    0.000000\n",
            "0                                      remainder__Age   -0.000943\n",
            "13                    remainder__ work is meaningful    -0.000943\n",
            "3                                remainder__Education   -0.001887\n",
            "5                             remainder__working hour   -0.001887\n",
            "1                                   remainder__Gender   -0.008491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/Permutation-based-entropy/new%20neumerical.csv')  # Replace with your file path\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = data[\"TOI (turnover intention)\"]\n",
        "\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Handle negative importance values by explanation\n",
        "importance_scores[\"Remark\"] = importance_scores[\"Importance\"].apply(\n",
        "    lambda x: \"May introduce noise\" if x < 0 else \"Useful\"\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "print(importance_scores)\n",
        "\n",
        "# Optionally, save the feature importance rankings to a CSV\n",
        "importance_scores.to_csv('feature_importance_ranking.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9MXSrXKVHQX",
        "outputId": "54dc876d-427d-4abc-f852-4324bb2a2aab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance  \\\n",
            "6               remainder__satisfaction with workload    0.051887   \n",
            "9   remainder__ satisfied with career and  opportu...    0.021698   \n",
            "7                 remainder__satisfied  compensation     0.019811   \n",
            "11            remainder__ monthly average expenditure    0.009434   \n",
            "15  remainder__ mentally well and do not have anxi...    0.006604   \n",
            "14                        remainder__family supports     0.002830   \n",
            "2                              remainder__Work tenure    0.001887   \n",
            "8           remainder__ good relationship with peers     0.001887   \n",
            "10          remainder__satisfied with job profession     0.000943   \n",
            "4                             remainder__Job position    0.000000   \n",
            "12      remainder__ satisfied with  work-life balance    0.000000   \n",
            "0                                      remainder__Age   -0.000943   \n",
            "13                    remainder__ work is meaningful    -0.000943   \n",
            "3                                remainder__Education   -0.001887   \n",
            "5                             remainder__working hour   -0.001887   \n",
            "1                                   remainder__Gender   -0.008491   \n",
            "\n",
            "                 Remark  \n",
            "6                Useful  \n",
            "9                Useful  \n",
            "7                Useful  \n",
            "11               Useful  \n",
            "15               Useful  \n",
            "14               Useful  \n",
            "2                Useful  \n",
            "8                Useful  \n",
            "10               Useful  \n",
            "4                Useful  \n",
            "12               Useful  \n",
            "0   May introduce noise  \n",
            "13  May introduce noise  \n",
            "3   May introduce noise  \n",
            "5   May introduce noise  \n",
            "1   May introduce noise  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"mentally well and do not have anxity\",\n",
        "    \"family supports\",\n",
        "    \"satisfied with career and  opportunity\",\n",
        "    \"Work tenure\",\n",
        "    \"monthly average expenditure \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "# Step 1: Normalize the Decision Matrix\n",
        "def normalize(X):\n",
        "    norm_factors = np.sqrt(np.sum(X**2, axis=0))\n",
        "    return X / norm_factors\n",
        "\n",
        "normalized_data = normalize(data)\n",
        "\n",
        "# Step 2: Calculate Proportions (p_ij)\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Step 3: Calculate Entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Step 4: Calculate Weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "\n",
        "# Step 5: Combine Weights\n",
        "alpha, beta = 0.5, 0.5  # Equal weight contributions\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Step 6: Compile Results into a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Display the results\n",
        "print(\"Entropy Weight Results:\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqpL7gLSJW8b",
        "outputId": "3d4c2d34-0d93-4e34-e138-c98245236bfb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy Weight Results:\n",
            "                                        Entropy (e_j)  Weight w_e1(j)  \\\n",
            "satisfied  compensation                      0.957687        0.018597   \n",
            "satisfaction with workload                   0.981912        0.007950   \n",
            "satisfied with job profession                0.982542        0.007673   \n",
            "mentally well and do not have anxity         0.469992        0.232939   \n",
            "family supports                              0.826744        0.076146   \n",
            "satisfied with career and  opportunity       0.301030        0.307198   \n",
            "Work tenure                                  0.939794        0.026461   \n",
            "monthly average expenditure                  0.301030        0.307198   \n",
            "good relationship with peers                 0.985212        0.006499   \n",
            "Job position                                 0.978749        0.009340   \n",
            "\n",
            "                                        Weight w_e2(j)  Final Weight w_e(j)  \n",
            "satisfied  compensation                       0.064606             0.041601  \n",
            "satisfaction with workload                    0.063012             0.035481  \n",
            "satisfied with job profession                 0.062972             0.035322  \n",
            "mentally well and do not have anxity          0.131646             0.182293  \n",
            "family supports                               0.074839             0.075492  \n",
            "satisfied with career and  opportunity        0.205536             0.256367  \n",
            "Work tenure                                   0.065836             0.046148  \n",
            "monthly average expenditure                   0.205536             0.256367  \n",
            "good relationship with peers                  0.062801             0.034650  \n",
            "Job position                                  0.063216             0.036278  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQfl992oX2hw",
        "outputId": "14a45c35-9c9a-441d-923a-288b4866534c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/244.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"mentally well and do not have anxity\",\n",
        "    \"family supports\",\n",
        "    \"satisfied with career and  opportunity\",\n",
        "    \"Work tenure\",\n",
        "    \"monthly average expenditure \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "\n",
        "# Normalize the data\n",
        "normalized_data = data / np.sqrt(np.sum(data**2, axis=0))\n",
        "\n",
        "# Calculate proportions\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Calculate entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Calculate weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "alpha, beta = 0.5, 0.5\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Recreate the results DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Recreate the document\n",
        "doc = Document()\n",
        "doc.add_heading(\"Entropy Weight Results\", level=1)\n",
        "\n",
        "# Add a table with the results\n",
        "table = doc.add_table(rows=1, cols=5)\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Add headers to the table\n",
        "headers = [\"Column Name\", \"Entropy (e_j)\", \"Weight w_e1(j)\", \"Weight w_e2(j)\", \"Final Weight w_e(j)\"]\n",
        "header_cells = table.rows[0].cells\n",
        "for i, header in enumerate(headers):\n",
        "    header_cells[i].text = header\n",
        "\n",
        "# Add rows to the table\n",
        "for idx, row in results.iterrows():\n",
        "    row_cells = table.add_row().cells\n",
        "    row_cells[0].text = idx  # Column Name\n",
        "    row_cells[1].text = f\"{row['Entropy (e_j)']:.6f}\"\n",
        "    row_cells[2].text = f\"{row['Weight w_e1(j)']:.6f}\"\n",
        "    row_cells[3].text = f\"{row['Weight w_e2(j)']:.6f}\"\n",
        "    row_cells[4].text = f\"{row['Final Weight w_e(j)']:.6f}\"\n",
        "\n",
        "# Save the document\n",
        "file_path = \"Entropy_Weight_Results.docx\"\n",
        "doc.save(file_path)\n",
        "\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eKmYBwEwLO2Y",
        "outputId": "e98dd164-5b9d-48bb-a79d-a5440849e347"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Entropy_Weight_Results.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/Permutation-based-entropy/new%20neumerical.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        " # Drop irrelevant/low-importance columns\n",
        "columns_to_drop = [\n",
        "    ' satisfied with  work-life balance',\n",
        "    'Age',\n",
        "    ' work is meaningful ',\n",
        "    'Education',\n",
        "    'working hour',\n",
        "    'Gender'\n",
        "]\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "# Extract the decision matrix (assuming all numerical columns after the first one)\n",
        "decision_matrix = data.iloc[:, 1:].to_numpy()  # Adjust index based on column arrangement\n",
        "\n",
        "# Updated weights for TOPSIS based on the new columns and final weights\n",
        "weights = np.array([\n",
        "    0.041601,  # satisfied compensation\n",
        "    0.035481,  # satisfaction with workload\n",
        "    0.035322,  # satisfied with job profession\n",
        "    0.182293,  # mentally well and do not have anxity\n",
        "    0.075492,  # family supports\n",
        "    0.256367,  # satisfied with career and  opportunity\n",
        "    0.046148,  # Work tenure\n",
        "    0.256367,  # monthly average expenditure\n",
        "    0.034650,  # good relationship with peers\n",
        "    0.036278   # Job position\n",
        "])\n",
        "# Define the TOPSIS method\n",
        "def topsis(decision_matrix, weights):\n",
        "    # Normalize the decision matrix\n",
        "    norm_factors = np.sqrt(np.sum(decision_matrix**2, axis=0))\n",
        "    normalized_matrix = decision_matrix / norm_factors\n",
        "\n",
        "    # Apply weights\n",
        "    weighted_matrix = normalized_matrix * weights\n",
        "\n",
        "    # Determine positive and negative ideals\n",
        "    positive_ideal = np.max(weighted_matrix, axis=0)\n",
        "    negative_ideal = np.min(weighted_matrix, axis=0)\n",
        "\n",
        "    # Calculate distances to the positive and negative ideals\n",
        "    distance_positive = np.sqrt(np.sum((weighted_matrix - positive_ideal) ** 2, axis=1))\n",
        "    distance_negative = np.sqrt(np.sum((weighted_matrix - negative_ideal) ** 2, axis=1))\n",
        "\n",
        "    # Calculate closeness coefficients\n",
        "    closeness_coefficients = distance_negative / (distance_positive + distance_negative)\n",
        "    return closeness_coefficients\n",
        "\n",
        "# Compute TOPSIS closeness coefficients\n",
        "closeness_coefficients = topsis(decision_matrix, weights)\n",
        "\n",
        "# Categorize employees based on closeness coefficients\n",
        "maxR = closeness_coefficients.max()\n",
        "minR = closeness_coefficients.min()\n",
        "Nclass = 3  # Number of categories\n",
        "D = (maxR - minR) / Nclass\n",
        "\n",
        "distressed_range = (minR, minR + D)\n",
        "behavioral_range = (minR + D, minR + 2 * D)\n",
        "enthusiastic_range = (minR + 2 * D, maxR)\n",
        "\n",
        "# Assign categories based on the ranges\n",
        "categories_named = []\n",
        "for cc in closeness_coefficients:\n",
        "    if distressed_range[0] <= cc <= distressed_range[1]:\n",
        "        categories_named.append(\"Distressed\")\n",
        "    elif behavioral_range[0] < cc <= behavioral_range[1]:\n",
        "        categories_named.append(\"Behavioral\")\n",
        "    elif enthusiastic_range[0] < cc <= enthusiastic_range[1]:\n",
        "        categories_named.append(\"Enthusiastic\")\n",
        "\n",
        "# Add results to the original dataset\n",
        "data['Closeness Coefficient'] = closeness_coefficients\n",
        "data['Category'] = categories_named\n",
        "\n",
        "# Save the updated dataset\n",
        "output_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtmaWOQMBwd",
        "outputId": "1037f557-ad29-4fca-b586-da0398ebceac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset saved to updated_employee_categorization_new_weights.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded dataset with categorization\n",
        "file_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Split the data into three categories\n",
        "distressed_data = data[data['Category'] == 'Distressed']\n",
        "behavioral_data = data[data['Category'] == 'Behavioral']\n",
        "enthusiastic_data = data[data['Category'] == 'Enthusiastic']\n",
        "\n",
        "# Save each category into separate files\n",
        "distressed_file = 'distressed_employees.xlsx'\n",
        "behavioral_file = 'behavioral_employees.xlsx'\n",
        "enthusiastic_file = 'enthusiastic_employees.xlsx'\n",
        "\n",
        "distressed_data.to_excel(distressed_file, index=False)\n",
        "behavioral_data.to_excel(behavioral_file, index=False)\n",
        "enthusiastic_data.to_excel(enthusiastic_file, index=False)\n",
        "\n",
        "distressed_file, behavioral_file, enthusiastic_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MujnfCmEe9Dc",
        "outputId": "b6c439b0-8355-4465-fda7-33b8f83b237e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('distressed_employees.xlsx',\n",
              " 'behavioral_employees.xlsx',\n",
              " 'enthusiastic_employees.xlsx')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'distressed_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\")\n",
        "print(data['TOI (turnover intention)'].value_counts())\n",
        "\n",
        "# Target sample sizes\n",
        "target_class_0 = 2764\n",
        "target_class_1 = 2763\n",
        "\n",
        "# Oversample Class 0\n",
        "if len(class_0) > 0:\n",
        "    class_0_balanced = resample(\n",
        "        class_0,\n",
        "        replace=True,\n",
        "        n_samples=target_class_0,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 0 has zero samples. Skipping resampling.\")\n",
        "    class_0_balanced = pd.DataFrame()\n",
        "\n",
        "# Oversample Class 1\n",
        "if len(class_1) > 0:\n",
        "    class_1_balanced = resample(\n",
        "        class_1,\n",
        "        replace=True,\n",
        "        n_samples=target_class_1,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 1 has zero samples. Skipping resampling.\")\n",
        "    class_1_balanced = pd.DataFrame()\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "output_path = 'balanced_dataset-distressed.csv'\n",
        "balanced_data.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(\"Class Distribution After Balancing:\")\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n",
        "print(f\"Balanced dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kxP-IlOcHAi",
        "outputId": "e4c00e31-5f21-4131-d010-e16705b44a40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution Before Balancing:\n",
            "TOI (turnover intention)\n",
            "1    121\n",
            "0      9\n",
            "Name: count, dtype: int64\n",
            "Class Distribution After Balancing:\n",
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to balanced_dataset-distressed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-distressed.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,           # Fewer trees\n",
        "    max_depth=3,               # Limit depth\n",
        "    min_samples_split=10,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDlBVseOoIi",
        "outputId": "5df2fc79-f5e1-4d37-d4f1-d145fab43ff7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[401 152]\n",
            " [ 35 518]]\n",
            "\n",
            "Accuracy: 0.8309\n",
            "Precision: 0.7731\n",
            "Recall: 0.9367\n",
            "F1-Score: 0.8471\n",
            "MCC: 0.6772\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.73      0.81       553\n",
            "           1       0.77      0.94      0.85       553\n",
            "\n",
            "    accuracy                           0.83      1106\n",
            "   macro avg       0.85      0.83      0.83      1106\n",
            "weighted avg       0.85      0.83      0.83      1106\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'behavioral_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-behavioral.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYuuXnOegLt",
        "outputId": "cc18fa24-00ef-4733-cfd9-bb07465a4531"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-behavioral.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGuDNlWeO90A",
        "outputId": "2ff6038a-96a2-48ed-da61-08e1413ac67f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[538  13]\n",
            " [ 43 507]]\n",
            "\n",
            "Accuracy: 0.9491\n",
            "Precision: 0.9750\n",
            "Recall: 0.9218\n",
            "F1-Score: 0.9477\n",
            "MCC: 0.8996\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.98      0.95       551\n",
            "           1       0.97      0.92      0.95       550\n",
            "\n",
            "    accuracy                           0.95      1101\n",
            "   macro avg       0.95      0.95      0.95      1101\n",
            "weighted avg       0.95      0.95      0.95      1101\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'enthusiastic_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-enthusiastic.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1P55t_xPtX7",
        "outputId": "47b84439-75f5-4aef-ef28-bec72e49d258"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-enthusiastic.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ssS5D_PthG",
        "outputId": "6118ce80-3069-4f68-f671-4bc80290450a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[505  48]\n",
            " [ 39 513]]\n",
            "\n",
            "Accuracy: 0.9213\n",
            "Precision: 0.9144\n",
            "Recall: 0.9293\n",
            "F1-Score: 0.9218\n",
            "MCC: 0.8426\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92       553\n",
            "           1       0.91      0.93      0.92       552\n",
            "\n",
            "    accuracy                           0.92      1105\n",
            "   macro avg       0.92      0.92      0.92      1105\n",
            "weighted avg       0.92      0.92      0.92      1105\n",
            "\n"
          ]
        }
      ]
    }
  ]
}