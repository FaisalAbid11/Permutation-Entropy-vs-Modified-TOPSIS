{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xuf-ns4Hw10B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/Permutation-based-entropy/new%20neumerical.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to transform all categorical columns to numerical values\n",
        "def transform_categorical_to_numerical(df):\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply Label Encoding to all categorical columns\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        df[col + '_Encoded'] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Transform the dataset\n",
        "data_transformed = transform_categorical_to_numerical(data)\n",
        "\n",
        "# Save or display the transformed dataset\n",
        "print(data_transformed.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBmgKQbLEkwI",
        "outputId": "861316c7-370e-411e-d7bf-4902633d4d48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Gender  Work tenure  Education  Job position  working hour  \\\n",
            "0    2       1            2          3             1             2   \n",
            "1    2       1            2          3             2             1   \n",
            "2    4       2            3          3             1             3   \n",
            "3    3       1            3          4             2             2   \n",
            "4    3       1            3          4             3             1   \n",
            "\n",
            "   satisfaction with workload  satisfied  compensation   \\\n",
            "0                           4                         3   \n",
            "1                           2                         2   \n",
            "2                           1                         2   \n",
            "3                           2                         2   \n",
            "4                           2                         3   \n",
            "\n",
            "    good relationship with peers    satisfied with career and  opportunity  \\\n",
            "0                               4                                        4   \n",
            "1                               2                                        2   \n",
            "2                               2                                        2   \n",
            "3                               2                                        2   \n",
            "4                               2                                        2   \n",
            "\n",
            "   satisfied with job profession    monthly average expenditure  \\\n",
            "0                               4                             2   \n",
            "1                               2                             4   \n",
            "2                               2                             3   \n",
            "3                               2                             4   \n",
            "4                               3                             3   \n",
            "\n",
            "    satisfied with  work-life balance   work is meaningful   family supports   \\\n",
            "0                                   4                     4                 4   \n",
            "1                                   2                     2                 2   \n",
            "2                                   2                     2                 2   \n",
            "3                                   2                     2                 2   \n",
            "4                                   2                     3                 2   \n",
            "\n",
            "    mentally well and do not have anxiety   TOI (turnover intention)  \n",
            "0                                        4                         0  \n",
            "1                                        2                         1  \n",
            "2                                        2                         1  \n",
            "3                                        2                         0  \n",
            "4                                        2                         1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('new neumerical (3).csv')\n",
        "# Separate features and target\n",
        "X = cleaned_data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = cleaned_data[\"TOI (turnover intention)\"]\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Display the top features\n",
        "print(importance_scores.head(17))  # Top 5 features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auj8nRi-PbNm",
        "outputId": "55ec700d-432d-4264-b642-b17331664746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance\n",
            "6               remainder__satisfaction with workload    0.051887\n",
            "9   remainder__ satisfied with career and  opportu...    0.021698\n",
            "7                 remainder__satisfied  compensation     0.019811\n",
            "11            remainder__ monthly average expenditure    0.009434\n",
            "15  remainder__ mentally well and do not have anxi...    0.006604\n",
            "14                        remainder__family supports     0.002830\n",
            "2                              remainder__Work tenure    0.001887\n",
            "8           remainder__ good relationship with peers     0.001887\n",
            "10          remainder__satisfied with job profession     0.000943\n",
            "4                             remainder__Job position    0.000000\n",
            "12      remainder__ satisfied with  work-life balance    0.000000\n",
            "0                                      remainder__Age   -0.000943\n",
            "13                    remainder__ work is meaningful    -0.000943\n",
            "3                                remainder__Education   -0.001887\n",
            "5                             remainder__working hour   -0.001887\n",
            "1                                   remainder__Gender   -0.008491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('new neumerical (3).csv')  # Replace with your file path\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = cleaned_data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = cleaned_data[\"TOI (turnover intention)\"]\n",
        "\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Handle negative importance values by explanation\n",
        "importance_scores[\"Remark\"] = importance_scores[\"Importance\"].apply(\n",
        "    lambda x: \"May introduce noise\" if x < 0 else \"Useful\"\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "print(importance_scores)\n",
        "\n",
        "# Optionally, save the feature importance rankings to a CSV\n",
        "importance_scores.to_csv('feature_importance_ranking.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9MXSrXKVHQX",
        "outputId": "5f1ac0ae-73ac-4595-95b9-334218427d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance  \\\n",
            "6               remainder__satisfaction with workload    0.051887   \n",
            "9   remainder__ satisfied with career and  opportu...    0.021698   \n",
            "7                 remainder__satisfied  compensation     0.019811   \n",
            "11            remainder__ monthly average expenditure    0.009434   \n",
            "15  remainder__ mentally well and do not have anxi...    0.006604   \n",
            "14                        remainder__family supports     0.002830   \n",
            "2                              remainder__Work tenure    0.001887   \n",
            "8           remainder__ good relationship with peers     0.001887   \n",
            "10          remainder__satisfied with job profession     0.000943   \n",
            "4                             remainder__Job position    0.000000   \n",
            "12      remainder__ satisfied with  work-life balance    0.000000   \n",
            "0                                      remainder__Age   -0.000943   \n",
            "13                    remainder__ work is meaningful    -0.000943   \n",
            "3                                remainder__Education   -0.001887   \n",
            "5                             remainder__working hour   -0.001887   \n",
            "1                                   remainder__Gender   -0.008491   \n",
            "\n",
            "                 Remark  \n",
            "6                Useful  \n",
            "9                Useful  \n",
            "7                Useful  \n",
            "11               Useful  \n",
            "15               Useful  \n",
            "14               Useful  \n",
            "2                Useful  \n",
            "8                Useful  \n",
            "10               Useful  \n",
            "4                Useful  \n",
            "12               Useful  \n",
            "0   May introduce noise  \n",
            "13  May introduce noise  \n",
            "3   May introduce noise  \n",
            "5   May introduce noise  \n",
            "1   May introduce noise  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"working hour\",\n",
        "    \"Work tenure\",\n",
        "    \"Gender\",\n",
        "    \"Education\",\n",
        "    \"Gender \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "\n",
        "# Step 1: Normalize the Decision Matrix\n",
        "def normalize(X):\n",
        "    norm_factors = np.sqrt(np.sum(X**2, axis=0))\n",
        "    return X / norm_factors\n",
        "\n",
        "normalized_data = normalize(data)\n",
        "\n",
        "# Step 2: Calculate Proportions (p_ij)\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Step 3: Calculate Entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Step 4: Calculate Weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "\n",
        "# Step 5: Combine Weights\n",
        "alpha, beta = 0.5, 0.5  # Equal weight contributions\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Step 6: Compile Results into a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Display the results\n",
        "print(\"Entropy Weight Results:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZJs0GhvT0a",
        "outputId": "2d3cf5bc-4254-4e1f-b4e5-138d0d6b4847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy Weight Results:\n",
            "                               Entropy (e_j)  Weight w_e1(j)  Weight w_e2(j)  \\\n",
            "satisfied  compensation             0.957687        0.018597        0.064606   \n",
            "satisfaction with workload          0.981912        0.007950        0.063012   \n",
            "satisfied with job profession       0.982542        0.007673        0.062972   \n",
            "working hour                        0.469992        0.232939        0.131646   \n",
            "Work tenure                         0.826744        0.076146        0.074839   \n",
            "Gender                              0.301030        0.307198        0.205536   \n",
            "Education                           0.939794        0.026461        0.065836   \n",
            "Gender                              0.301030        0.307198        0.205536   \n",
            "good relationship with peers        0.985212        0.006499        0.062801   \n",
            "Job position                        0.978749        0.009340        0.063216   \n",
            "\n",
            "                               Final Weight w_e(j)  \n",
            "satisfied  compensation                   0.041601  \n",
            "satisfaction with workload                0.035481  \n",
            "satisfied with job profession             0.035322  \n",
            "working hour                              0.182293  \n",
            "Work tenure                               0.075492  \n",
            "Gender                                    0.256367  \n",
            "Education                                 0.046148  \n",
            "Gender                                    0.256367  \n",
            "good relationship with peers              0.034650  \n",
            "Job position                              0.036278  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQfl992oX2hw",
        "outputId": "6317d9e8-26df-436b-aec0-b1c9dec5e669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'new neumerical (3).csv'  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop irrelevant/low-importance columns\n",
        "columns_to_drop = [\n",
        "    ' satisfied with  work-life balance',\n",
        "    'Age',\n",
        "    ' work is meaningful ',\n",
        "    'Education',\n",
        "    'working hour',\n",
        "    'Gender'\n",
        "]\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Extract the decision matrix (assuming 1st column is an ID)\n",
        "decision_matrix = data.iloc[:, 1:].to_numpy()\n",
        "\n",
        "# Check shape\n",
        "print(\"Decision matrix shape:\", decision_matrix.shape)  # Should be (526, 10)\n",
        "\n",
        "# Updated weights for remaining 10 features (make sure these are the correct 10 feature names)\n",
        "weights = np.array([\n",
        "    0.041601,  # satisfied compensation\n",
        "    0.035481,  # satisfaction with workload\n",
        "    0.035322,  # satisfied with job profession\n",
        "    0.075492,  # Work tenure\n",
        "    0.046148,  # Possibly new feature kept\n",
        "    0.034650,  # good relationship with peers\n",
        "    0.036278,  # Job position\n",
        "    0.090000,  # Fill with actual corresponding values\n",
        "    0.080000,  # ...\n",
        "    0.065000   # ...\n",
        "])\n",
        "\n",
        "# Normalize weights to sum to 1\n",
        "weights = weights / np.sum(weights)\n",
        "\n",
        "# Define the TOPSIS method\n",
        "def topsis(decision_matrix, weights):\n",
        "    # Normalize the decision matrix\n",
        "    norm_factors = np.sqrt(np.sum(decision_matrix**2, axis=0))\n",
        "    normalized_matrix = decision_matrix / norm_factors\n",
        "\n",
        "    # Apply weights\n",
        "    weighted_matrix = normalized_matrix * weights\n",
        "\n",
        "    # Determine positive and negative\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifHAZpVa5V8_",
        "outputId": "45babd3c-ffb5-49ae-b341-21d874c8c7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision matrix shape: (526, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the categorized dataset\n",
        "file_path = 'updated_employee_categorization_new_weights.xlsx'  # Update the file name if needed\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Split the data into three categories\n",
        "distressed = data[data['Category'] == 'Distressed']\n",
        "behavioral = data[data['Category'] == 'Behavioral']\n",
        "enthusiastic = data[data['Category'] == 'Enthusiastic']\n",
        "\n",
        "# Save each category into a separate file\n",
        "distressed.to_excel('distressed_employees.xlsx', index=False)\n",
        "behavioral.to_excel('behavioral_employees.xlsx', index=False)\n",
        "enthusiastic.to_excel('enthusiastic_employees.xlsx', index=False)\n",
        "\n",
        "print(\"Files saved:\")\n",
        "print(\"  - distressed_employees.xlsx\")\n",
        "print(\"  - behavioral_employees.xlsx\")\n",
        "print(\"  - enthusiastic_employees.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuqjbpq3Z4iJ",
        "outputId": "a549f1d6-9668-468c-9b1d-a7746fc3062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved:\n",
            "  - distressed_employees.xlsx\n",
            "  - behavioral_employees.xlsx\n",
            "  - enthusiastic_employees.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'distressed_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\")\n",
        "print(data['TOI (turnover intention)'].value_counts())\n",
        "\n",
        "# Target sample sizes\n",
        "target_class_0 = 2764\n",
        "target_class_1 = 2763\n",
        "\n",
        "# Oversample Class 0\n",
        "if len(class_0) > 0:\n",
        "    class_0_balanced = resample(\n",
        "        class_0,\n",
        "        replace=True,\n",
        "        n_samples=target_class_0,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 0 has zero samples. Skipping resampling.\")\n",
        "    class_0_balanced = pd.DataFrame()\n",
        "\n",
        "# Oversample Class 1\n",
        "if len(class_1) > 0:\n",
        "    class_1_balanced = resample(\n",
        "        class_1,\n",
        "        replace=True,\n",
        "        n_samples=target_class_1,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 1 has zero samples. Skipping resampling.\")\n",
        "    class_1_balanced = pd.DataFrame()\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "output_path = 'balanced_dataset-distressed.csv'\n",
        "balanced_data.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(\"Class Distribution After Balancing:\")\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n",
        "print(f\"Balanced dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kxP-IlOcHAi",
        "outputId": "06603e99-4d43-46b8-db92-2ff3d21471c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution Before Balancing:\n",
            "TOI (turnover intention)\n",
            "1    116\n",
            "0     24\n",
            "Name: count, dtype: int64\n",
            "Class Distribution After Balancing:\n",
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to balanced_dataset-distressed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-distressed.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=50,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=10,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDlBVseOoIi",
        "outputId": "ad9c7985-0563-4f86-c4f9-649abb1b73f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[528  23]\n",
            " [ 22 529]]\n",
            "\n",
            "Accuracy: 0.9592\n",
            "Precision: 0.9583\n",
            "Recall: 0.9601\n",
            "F1-Score: 0.9592\n",
            "MCC: 0.9183\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       551\n",
            "           1       0.96      0.96      0.96       551\n",
            "\n",
            "    accuracy                           0.96      1102\n",
            "   macro avg       0.96      0.96      0.96      1102\n",
            "weighted avg       0.96      0.96      0.96      1102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'behavioral_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-behavioral.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYuuXnOegLt",
        "outputId": "b36e56d6-61d4-4ef8-df72-8eb5a14d1fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-behavioral.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=50,            # Fewer trees (less ensemble power)\n",
        "    max_depth=10,                # Shallow trees to underfit\n",
        "    min_samples_split=20,       # Higher value = less splits\n",
        "    min_samples_leaf=20,        # Higher value = fewer leaves, more generalization\n",
        "    max_features='sqrt',        # Reduce number of features considered at each split\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGuDNlWeO90A",
        "outputId": "8fcfff87-ce04-47eb-aa96-9e8a705339b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[528  25]\n",
            " [ 31 521]]\n",
            "\n",
            "Accuracy: 0.9493\n",
            "Precision: 0.9542\n",
            "Recall: 0.9438\n",
            "F1-Score: 0.9490\n",
            "MCC: 0.8987\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95       553\n",
            "           1       0.95      0.94      0.95       552\n",
            "\n",
            "    accuracy                           0.95      1105\n",
            "   macro avg       0.95      0.95      0.95      1105\n",
            "weighted avg       0.95      0.95      0.95      1105\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'enthusiastic_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-enthusiastic.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1P55t_xPtX7",
        "outputId": "8eaeb51e-bbad-4d80-eee4-3dc4821e14ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-enthusiastic.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=50,            # Fewer trees (less ensemble power)\n",
        "    max_depth=3,                # Shallow trees to underfit\n",
        "    min_samples_split=10,       # Higher value = less splits\n",
        "    min_samples_leaf=20,        # Higher value = fewer leaves, more generalization\n",
        "    max_features='sqrt',        # Reduce number of features considered at each split\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ssS5D_PthG",
        "outputId": "a3e36ec7-a3dd-4331-b923-28cb8fcd8467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[528  24]\n",
            " [ 47 505]]\n",
            "\n",
            "Accuracy: 0.9357\n",
            "Precision: 0.9546\n",
            "Recall: 0.9149\n",
            "F1-Score: 0.9343\n",
            "MCC: 0.8721\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94       552\n",
            "           1       0.95      0.91      0.93       552\n",
            "\n",
            "    accuracy                           0.94      1104\n",
            "   macro avg       0.94      0.94      0.94      1104\n",
            "weighted avg       0.94      0.94      0.94      1104\n",
            "\n"
          ]
        }
      ]
    }
  ]
}