{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "08uLFjmiPOnY",
        "outputId": "46f2bbf9-f722-4491-a0b7-772f215d5522"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef9929e7-e4f1-471c-b3ac-e535f36e80aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef9929e7-e4f1-471c-b3ac-e535f36e80aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Fully_Numerical_Dataset.csv to Fully_Numerical_Dataset.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Fully_Numerical_Dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to transform all categorical columns to numerical values\n",
        "def transform_categorical_to_numerical(df):\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply Label Encoding to all categorical columns\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        df[col + '_Encoded'] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Transform the dataset\n",
        "data_transformed = transform_categorical_to_numerical(data)\n",
        "\n",
        "# Save or display the transformed dataset\n",
        "print(data_transformed.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBmgKQbLEkwI",
        "outputId": "1fefd551-4144-4306-fc5a-f415ba5b6048"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   a  Gender  Work tenure  Education  Job position  Unnamed: 5  working hour  \\\n",
            "0  1       1            0          4             8           0             3   \n",
            "1  1       0            1          1             5           0             3   \n",
            "2  3       0            3          2             6           0             0   \n",
            "3  4       0            3          2             6           0             2   \n",
            "4  1       0            0          1             4           0             0   \n",
            "\n",
            "   satisfaction with workload  satisfied  compensation   \\\n",
            "0                           3                         2   \n",
            "1                           2                         3   \n",
            "2                           2                         3   \n",
            "3                           2                         3   \n",
            "4                           2                         1   \n",
            "\n",
            "    good relationship with peers   ...  \\\n",
            "0                               1  ...   \n",
            "1                               2  ...   \n",
            "2                               2  ...   \n",
            "3                               2  ...   \n",
            "4                               3  ...   \n",
            "\n",
            "   2. In last 3 months, I am searching for  an alternative job (Dalam 3 bulan terakhir, saya mencari alternatif pekerjaan lain)  \\\n",
            "0                                                  2                                                                              \n",
            "1                                                  1                                                                              \n",
            "2                                                  3                                                                              \n",
            "3                                                  0                                                                              \n",
            "4                                                  1                                                                              \n",
            "\n",
            "   3. In last 3 months, I have low work motivation (Dalam 3 bulan terakhir, motivasi kerja saya rendah)  \\\n",
            "0                                                  2                                                      \n",
            "1                                                  1                                                      \n",
            "2                                                  3                                                      \n",
            "3                                                  1                                                      \n",
            "4                                                  1                                                      \n",
            "\n",
            "   TOI (turnover intention)  a_Encoded  Gender_Encoded  Work tenure_Encoded  \\\n",
            "0                         1          1               1                    0   \n",
            "1                         0          1               0                    1   \n",
            "2                         1          3               0                    3   \n",
            "3                         0          4               0                    3   \n",
            "4                         0          1               0                    0   \n",
            "\n",
            "   Education_Encoded  Job position_Encoded  working hour_Encoded  \\\n",
            "0                  4                     9                     3   \n",
            "1                  1                     6                     3   \n",
            "2                  2                     7                     0   \n",
            "3                  2                     7                     2   \n",
            "4                  1                     5                     0   \n",
            "\n",
            "    monthly average expenditure_Encoded  \n",
            "0                                     2  \n",
            "1                                     1  \n",
            "2                                     3  \n",
            "3                                     3  \n",
            "4                                     1  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('Fully_Numerical_Dataset.csv')  # Replace with your file path\n",
        "\n",
        "# Drop irrelevant columns if any\n",
        "cleaned_data = data.drop(columns=[\"1. In last 3 months, I am thinking of quitting from this office/organization (Dalam 3 bulan terakhir, saya berpikir untuk berhenti dari kantor/organisasi ini)\", \"2. In last 3 months, I am searching for  an alternative job (Dalam 3 bulan terakhir, saya mencari alternatif pekerjaan lain)\",\"3. In last 3 months, I have low work motivation (Dalam 3 bulan terakhir, motivasi kerja saya rendah)\"], errors='ignore')  # Modify as needed\n",
        "\n",
        "# Separate features and target\n",
        "X = cleaned_data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = cleaned_data[\"TOI (turnover intention)\"]\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Display the top features\n",
        "print(importance_scores.head(17))  # Top 5 features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auj8nRi-PbNm",
        "outputId": "8aa08864-12b2-4d16-ead3-890c1c29ef76"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance\n",
            "8                 remainder__satisfied  compensation     0.032075\n",
            "7               remainder__satisfaction with workload    0.024528\n",
            "11          remainder__satisfied with job profession     0.016981\n",
            "22                    remainder__working hour_Encoded    0.016038\n",
            "19                     remainder__Work tenure_Encoded    0.010377\n",
            "18                          remainder__Gender_Encoded    0.008491\n",
            "2                              remainder__Work tenure    0.008491\n",
            "3                                remainder__Education    0.005660\n",
            "1                                   remainder__Gender    0.005660\n",
            "6                             remainder__working hour    0.005660\n",
            "9           remainder__ good relationship with peers     0.004717\n",
            "4                             remainder__Job position    0.003774\n",
            "16  remainder__ mentally well and do not have anxi...    0.003774\n",
            "20                       remainder__Education_Encoded    0.003774\n",
            "13      remainder__ satisfied with  work-life balance    0.000943\n",
            "10  remainder__ satisfied with career and  opportu...    0.000000\n",
            "5                               remainder__Unnamed: 5    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "ltY4xiCeHyo9",
        "outputId": "a5e1fc6c-2a6e-4f0c-f283-9d40f9f7fd36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cfde0e15-8ca9-4ef7-8aef-38b03e738e97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cfde0e15-8ca9-4ef7-8aef-38b03e738e97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving new neumerical.csv to new neumerical.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('new neumerical.csv')  # Replace with your file path\n",
        "\n",
        "# Drop irrelevant columns if any\n",
        "cleaned_data = data.drop(columns=[\"1. In last 3 months, I am thinking of quitting from this office/organization (Dalam 3 bulan terakhir, saya berpikir untuk berhenti dari kantor/organisasi ini)\", \"2. In last 3 months, I am searching for  an alternative job (Dalam 3 bulan terakhir, saya mencari alternatif pekerjaan lain)\",\"3. In last 3 months, I have low work motivation (Dalam 3 bulan terakhir, motivasi kerja saya rendah)\"], errors='ignore')  # Modify as needed\n",
        "\n",
        "# Separate features and target\n",
        "X = cleaned_data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = cleaned_data[\"TOI (turnover intention)\"]\n",
        "\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Handle negative importance values by explanation\n",
        "importance_scores[\"Remark\"] = importance_scores[\"Importance\"].apply(\n",
        "    lambda x: \"May introduce noise\" if x < 0 else \"Useful\"\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "print(importance_scores)\n",
        "\n",
        "# Optionally, save the feature importance rankings to a CSV\n",
        "importance_scores.to_csv('feature_importance_ranking.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9MXSrXKVHQX",
        "outputId": "f94632f2-e1fa-42b4-8ed2-d938065996a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Feature  Importance  Remark\n",
            "1      remainder__satisfaction with workload    0.083962  Useful\n",
            "0        remainder__satisfied  compensation     0.070755  Useful\n",
            "2  remainder__satisfied with job profession     0.045283  Useful\n",
            "3                    remainder__working hour    0.038679  Useful\n",
            "7                        remainder__Gender.1    0.014151  Useful\n",
            "4                     remainder__Work tenure    0.011321  Useful\n",
            "6                       remainder__Education    0.007547  Useful\n",
            "5                          remainder__Gender    0.004717  Useful\n",
            "8  remainder__ good relationship with peers     0.004717  Useful\n",
            "9                    remainder__Job position    0.004717  Useful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"working hour\",\n",
        "    \"Work tenure\",\n",
        "    \"Gender\",\n",
        "    \"Education\",\n",
        "    \"Gender \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "\n",
        "# Step 1: Normalize the Decision Matrix\n",
        "def normalize(X):\n",
        "    norm_factors = np.sqrt(np.sum(X**2, axis=0))\n",
        "    return X / norm_factors\n",
        "\n",
        "normalized_data = normalize(data)\n",
        "\n",
        "# Step 2: Calculate Proportions (p_ij)\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Step 3: Calculate Entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Step 4: Calculate Weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "\n",
        "# Step 5: Combine Weights\n",
        "alpha, beta = 0.5, 0.5  # Equal weight contributions\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Step 6: Compile Results into a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Display the results\n",
        "print(\"Entropy Weight Results:\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqpL7gLSJW8b",
        "outputId": "63f617ce-3a5e-45e5-dfac-dd3eef3a3feb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy Weight Results:\n",
            "                               Entropy (e_j)  Weight w_e1(j)  Weight w_e2(j)  \\\n",
            "satisfied  compensation             0.957687        0.018597        0.064606   \n",
            "satisfaction with workload          0.981912        0.007950        0.063012   \n",
            "satisfied with job profession       0.982542        0.007673        0.062972   \n",
            "working hour                        0.469992        0.232939        0.131646   \n",
            "Work tenure                         0.826744        0.076146        0.074839   \n",
            "Gender                              0.301030        0.307198        0.205536   \n",
            "Education                           0.939794        0.026461        0.065836   \n",
            "Gender                              0.301030        0.307198        0.205536   \n",
            "good relationship with peers        0.985212        0.006499        0.062801   \n",
            "Job position                        0.978749        0.009340        0.063216   \n",
            "\n",
            "                               Final Weight w_e(j)  \n",
            "satisfied  compensation                   0.041601  \n",
            "satisfaction with workload                0.035481  \n",
            "satisfied with job profession             0.035322  \n",
            "working hour                              0.182293  \n",
            "Work tenure                               0.075492  \n",
            "Gender                                    0.256367  \n",
            "Education                                 0.046148  \n",
            "Gender                                    0.256367  \n",
            "good relationship with peers              0.034650  \n",
            "Job position                              0.036278  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQfl992oX2hw",
        "outputId": "d260a8fc-90c2-49ae-e350-ddc00ff00f11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/244.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "# Create a Word document\n",
        "doc = Document()\n",
        "doc.add_heading('Entropy Weight Calculation Results', level=1)\n",
        "\n",
        "# Table data\n",
        "data = [\n",
        "    [\"Feature\", \"Entropy (e_j)\", \"Weight w_e1(j)\", \"Weight w_e2(j)\", \"Final Weight w_e(j)\"],\n",
        "    [\"Satisfaction with Workload\", \"0.991489\", \"0.068877\", \"0.124066\", \"0.096471\"],\n",
        "    [\"Satisfied with Career Opportunity\", \"0.996532\", \"0.028062\", \"0.123438\", \"0.075750\"],\n",
        "    [\"Satisfied Compensation\", \"0.982155\", \"0.144406\", \"0.125245\", \"0.134826\"],\n",
        "    [\"Good Relationship with Peers\", \"0.996229\", \"0.030515\", \"0.123476\", \"0.076995\"],\n",
        "    [\"Mentally Well and No Anxiety\", \"0.987792\", \"0.098791\", \"0.124530\", \"0.111661\"],\n",
        "    [\"Satisfied with Job Profession\", \"0.996532\", \"0.028062\", \"0.123438\", \"0.075750\"],\n",
        "    [\"Family Supports\", \"0.995503\", \"0.036392\", \"0.123566\", \"0.079979\"],\n",
        "    [\"Monthly Average Expenditure\", \"0.930193\", \"0.564895\", \"0.132241\", \"0.348568\"],\n",
        "]\n",
        "\n",
        "# Add table to the document\n",
        "table = doc.add_table(rows=1, cols=len(data[0]))\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Add headers\n",
        "hdr_cells = table.rows[0].cells\n",
        "for i, heading in enumerate(data[0]):\n",
        "    hdr_cells[i].text = heading\n",
        "\n",
        "# Add data rows\n",
        "for row_data in data[1:]:\n",
        "    row_cells = table.add_row().cells\n",
        "    for i, value in enumerate(row_data):\n",
        "        row_cells[i].text = value\n",
        "\n",
        "# Save the document\n",
        "doc.save(\"Entropy_Weight_Results.docx\")\n"
      ],
      "metadata": {
        "id": "ejdoBQT_VVsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"working hour\",\n",
        "    \"Work tenure\",\n",
        "    \"Gender\",\n",
        "    \"Education\",\n",
        "    \"Gender \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "\n",
        "# Normalize the data\n",
        "normalized_data = data / np.sqrt(np.sum(data**2, axis=0))\n",
        "\n",
        "# Calculate proportions\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Calculate entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Calculate weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "alpha, beta = 0.5, 0.5\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Recreate the results DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Recreate the document\n",
        "doc = Document()\n",
        "doc.add_heading(\"Entropy Weight Results\", level=1)\n",
        "\n",
        "# Add a table with the results\n",
        "table = doc.add_table(rows=1, cols=5)\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Add headers to the table\n",
        "headers = [\"Column Name\", \"Entropy (e_j)\", \"Weight w_e1(j)\", \"Weight w_e2(j)\", \"Final Weight w_e(j)\"]\n",
        "header_cells = table.rows[0].cells\n",
        "for i, header in enumerate(headers):\n",
        "    header_cells[i].text = header\n",
        "\n",
        "# Add rows to the table\n",
        "for idx, row in results.iterrows():\n",
        "    row_cells = table.add_row().cells\n",
        "    row_cells[0].text = idx  # Column Name\n",
        "    row_cells[1].text = f\"{row['Entropy (e_j)']:.6f}\"\n",
        "    row_cells[2].text = f\"{row['Weight w_e1(j)']:.6f}\"\n",
        "    row_cells[3].text = f\"{row['Weight w_e2(j)']:.6f}\"\n",
        "    row_cells[4].text = f\"{row['Final Weight w_e(j)']:.6f}\"\n",
        "\n",
        "# Save the document\n",
        "file_path = \"Entropy_Weight_Results.docx\"\n",
        "doc.save(file_path)\n",
        "\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "eKmYBwEwLO2Y",
        "outputId": "092b1906-9bfa-43dc-96e4-a7faf83ec36d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Entropy_Weight_Results.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "7y2QIkQ2ZpME",
        "outputId": "83b01930-41f8-4f0f-851f-b1549cc3ddcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57c74aaf-f7cf-4d35-9d3f-8758cfaefbf9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57c74aaf-f7cf-4d35-9d3f-8758cfaefbf9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving permutation based feature.csv to permutation based feature.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'new neumerical.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract the decision matrix (assuming all numerical columns after the first one)\n",
        "decision_matrix = data.iloc[:, 1:].to_numpy()  # Adjust index based on column arrangement\n",
        "\n",
        "# Updated weights for TOPSIS based on the new columns and final weights\n",
        "weights = np.array([\n",
        "    0.041601,  # satisfied compensation\n",
        "    0.035481,  # satisfaction with workload\n",
        "    0.035322,  # satisfied with job profession\n",
        "    0.182293,  # working hour\n",
        "    0.075492,  # Work tenure\n",
        "    0.256367,  # Gender\n",
        "    0.046148,  # Education\n",
        "    0.256367,  # Gender (duplicate column)\n",
        "    0.034650,  # good relationship with peers\n",
        "    0.036278   # Job position\n",
        "])\n",
        "\n",
        "# Define the TOPSIS method\n",
        "def topsis(decision_matrix, weights):\n",
        "    # Normalize the decision matrix\n",
        "    norm_factors = np.sqrt(np.sum(decision_matrix**2, axis=0))\n",
        "    normalized_matrix = decision_matrix / norm_factors\n",
        "\n",
        "    # Apply weights\n",
        "    weighted_matrix = normalized_matrix * weights\n",
        "\n",
        "    # Determine positive and negative ideals\n",
        "    positive_ideal = np.max(weighted_matrix, axis=0)\n",
        "    negative_ideal = np.min(weighted_matrix, axis=0)\n",
        "\n",
        "    # Calculate distances to the positive and negative ideals\n",
        "    distance_positive = np.sqrt(np.sum((weighted_matrix - positive_ideal) ** 2, axis=1))\n",
        "    distance_negative = np.sqrt(np.sum((weighted_matrix - negative_ideal) ** 2, axis=1))\n",
        "\n",
        "    # Calculate closeness coefficients\n",
        "    closeness_coefficients = distance_negative / (distance_positive + distance_negative)\n",
        "    return closeness_coefficients\n",
        "\n",
        "# Compute TOPSIS closeness coefficients\n",
        "closeness_coefficients = topsis(decision_matrix, weights)\n",
        "\n",
        "# Categorize employees based on closeness coefficients\n",
        "maxR = closeness_coefficients.max()\n",
        "minR = closeness_coefficients.min()\n",
        "Nclass = 3  # Number of categories\n",
        "D = (maxR - minR) / Nclass\n",
        "\n",
        "distressed_range = (minR, minR + D)\n",
        "behavioral_range = (minR + D, minR + 2 * D)\n",
        "enthusiastic_range = (minR + 2 * D, maxR)\n",
        "\n",
        "# Assign categories based on the ranges\n",
        "categories_named = []\n",
        "for cc in closeness_coefficients:\n",
        "    if distressed_range[0] <= cc <= distressed_range[1]:\n",
        "        categories_named.append(\"Distressed\")\n",
        "    elif behavioral_range[0] < cc <= behavioral_range[1]:\n",
        "        categories_named.append(\"Behavioral\")\n",
        "    elif enthusiastic_range[0] < cc <= enthusiastic_range[1]:\n",
        "        categories_named.append(\"Enthusiastic\")\n",
        "\n",
        "# Add results to the original dataset\n",
        "data['Closeness Coefficient'] = closeness_coefficients\n",
        "data['Category'] = categories_named\n",
        "\n",
        "# Save the updated dataset\n",
        "output_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtmaWOQMBwd",
        "outputId": "3d4443bc-299e-409f-f212-3290a4ceb70c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset saved to updated_employee_categorization_new_weights.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded dataset with categorization\n",
        "file_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Split the data into three categories\n",
        "distressed_data = data[data['Category'] == 'Distressed']\n",
        "behavioral_data = data[data['Category'] == 'Behavioral']\n",
        "enthusiastic_data = data[data['Category'] == 'Enthusiastic']\n",
        "\n",
        "# Save each category into separate files\n",
        "distressed_file = 'distressed_employees.xlsx'\n",
        "behavioral_file = 'behavioral_employees.xlsx'\n",
        "enthusiastic_file = 'enthusiastic_employees.xlsx'\n",
        "\n",
        "distressed_data.to_excel(distressed_file, index=False)\n",
        "behavioral_data.to_excel(behavioral_file, index=False)\n",
        "enthusiastic_data.to_excel(enthusiastic_file, index=False)\n",
        "\n",
        "distressed_file, behavioral_file, enthusiastic_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MujnfCmEe9Dc",
        "outputId": "f64ec0f3-0809-422d-f5d1-9b03e50a4eab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('distressed_employees.xlsx',\n",
              " 'behavioral_employees.xlsx',\n",
              " 'enthusiastic_employees.xlsx')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the categorized dataset\n",
        "file_path = 'updated_employee_categorization_permutation_weighted.xlsx'  # Update the file name if needed\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Split the data into three categories\n",
        "distressed = data[data['Category'] == 'Distressed']\n",
        "behavioral = data[data['Category'] == 'Behavioral']\n",
        "enthusiastic = data[data['Category'] == 'Enthusiastic']\n",
        "\n",
        "# Save each category into a separate file\n",
        "distressed.to_excel('distressed_employees.xlsx', index=False)\n",
        "behavioral.to_excel('behavioral_employees.xlsx', index=False)\n",
        "enthusiastic.to_excel('enthusiastic_employees.xlsx', index=False)\n",
        "\n",
        "print(\"Files saved:\")\n",
        "print(\"  - distressed_employees.xlsx\")\n",
        "print(\"  - behavioral_employees.xlsx\")\n",
        "print(\"  - enthusiastic_employees.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuqjbpq3Z4iJ",
        "outputId": "d3591baa-bd53-439b-b506-3f3ba84e7938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved:\n",
            "  - distressed_employees.xlsx\n",
            "  - behavioral_employees.xlsx\n",
            "  - enthusiastic_employees.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "49TslQb6eKfE",
        "outputId": "e4536bf4-e0d2-4685-e627-cc66011b1229"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c8fc62b-a2e4-404d-b8c3-6547f8b0b134\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c8fc62b-a2e4-404d-b8c3-6547f8b0b134\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving distressed_employees(1).xlsx to distressed_employees(1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'distressed_employees(1).xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\")\n",
        "print(data['TOI (turnover intention)'].value_counts())\n",
        "\n",
        "# Target sample sizes\n",
        "target_class_0 = 2764\n",
        "target_class_1 = 2763\n",
        "\n",
        "# Oversample Class 0\n",
        "if len(class_0) > 0:\n",
        "    class_0_balanced = resample(\n",
        "        class_0,\n",
        "        replace=True,\n",
        "        n_samples=target_class_0,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 0 has zero samples. Skipping resampling.\")\n",
        "    class_0_balanced = pd.DataFrame()\n",
        "\n",
        "# Oversample Class 1\n",
        "if len(class_1) > 0:\n",
        "    class_1_balanced = resample(\n",
        "        class_1,\n",
        "        replace=True,\n",
        "        n_samples=target_class_1,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 1 has zero samples. Skipping resampling.\")\n",
        "    class_1_balanced = pd.DataFrame()\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "output_path = 'balanced_dataset-distressed.csv'\n",
        "balanced_data.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(\"Class Distribution After Balancing:\")\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n",
        "print(f\"Balanced dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kxP-IlOcHAi",
        "outputId": "4756ecd0-89b5-4a2d-e4cc-d3e377efefd9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution Before Balancing:\n",
            "TOI (turnover intention)\n",
            "1    45\n",
            "0    17\n",
            "Name: count, dtype: int64\n",
            "Class Distribution After Balancing:\n",
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to balanced_dataset-distressed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-distressed.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDlBVseOoIi",
        "outputId": "1c9ed90a-a215-41a0-8afb-69d393544358"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[491  61]\n",
            " [ 25 526]]\n",
            "\n",
            "Accuracy: 0.9220\n",
            "Precision: 0.8961\n",
            "Recall: 0.9546\n",
            "F1-Score: 0.9244\n",
            "MCC: 0.8459\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92       552\n",
            "           1       0.90      0.95      0.92       551\n",
            "\n",
            "    accuracy                           0.92      1103\n",
            "   macro avg       0.92      0.92      0.92      1103\n",
            "weighted avg       0.92      0.92      0.92      1103\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "9-GZW2CP9BPA",
        "outputId": "b78963f2-e23b-4947-fed9-7cba7a978e5f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae5a68ae-8270-4dfa-b9ad-e3744028cee5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae5a68ae-8270-4dfa-b9ad-e3744028cee5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving behavioral_employees(1).xlsx to behavioral_employees(1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'behavioral_employees(1).xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-behavioral.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYuuXnOegLt",
        "outputId": "8d12e141-f24a-4f09-cde1-fa6e8c34e043"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-behavioral.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGuDNlWeO90A",
        "outputId": "e9a7d6bc-8304-4b75-aea2-c4cb49192ec8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[525  24]\n",
            " [ 77 472]]\n",
            "\n",
            "Accuracy: 0.9080\n",
            "Precision: 0.9516\n",
            "Recall: 0.8597\n",
            "F1-Score: 0.9033\n",
            "MCC: 0.8199\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91       549\n",
            "           1       0.95      0.86      0.90       549\n",
            "\n",
            "    accuracy                           0.91      1098\n",
            "   macro avg       0.91      0.91      0.91      1098\n",
            "weighted avg       0.91      0.91      0.91      1098\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "VC_iL1HPPtSc",
        "outputId": "04572436-a089-450e-e1c4-a10a60506b86"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9be89664-f4b0-408f-be0f-7e8e3cc5f016\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9be89664-f4b0-408f-be0f-7e8e3cc5f016\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving enthusiastic_employees(1).xlsx to enthusiastic_employees(1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'enthusiastic_employees(1).xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-enthusiastic.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1P55t_xPtX7",
        "outputId": "fd25767c-7725-4af6-f055-1826b6850f32"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-enthusiastic.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ssS5D_PthG",
        "outputId": "ec3d267e-246e-4075-f08c-f4595e611aef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[508  44]\n",
            " [ 37 514]]\n",
            "\n",
            "Accuracy: 0.9266\n",
            "Precision: 0.9211\n",
            "Recall: 0.9328\n",
            "F1-Score: 0.9270\n",
            "MCC: 0.8532\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.93       552\n",
            "           1       0.92      0.93      0.93       551\n",
            "\n",
            "    accuracy                           0.93      1103\n",
            "   macro avg       0.93      0.93      0.93      1103\n",
            "weighted avg       0.93      0.93      0.93      1103\n",
            "\n"
          ]
        }
      ]
    }
  ]
}