{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "08uLFjmiPOnY",
        "outputId": "fd04529c-53e8-4554-bc6e-d5e12d425ca9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-226fa17b-cffa-4b4b-83bf-d78b79376175\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-226fa17b-cffa-4b4b-83bf-d78b79376175\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving new neumerical.csv to new neumerical.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'new neumerical.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to transform all categorical columns to numerical values\n",
        "def transform_categorical_to_numerical(df):\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply Label Encoding to all categorical columns\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        df[col + '_Encoded'] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Transform the dataset\n",
        "data_transformed = transform_categorical_to_numerical(data)\n",
        "\n",
        "# Save or display the transformed dataset\n",
        "print(data_transformed.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBmgKQbLEkwI",
        "outputId": "fc3d50d8-f54d-4245-aefc-456d9ab12475"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Gender  Work tenure  Education  Job position  working hour  \\\n",
            "0    2       1            2          3             1             2   \n",
            "1    2       1            2          3             2             1   \n",
            "2    4       2            3          3             1             3   \n",
            "3    3       1            3          4             2             2   \n",
            "4    3       1            3          4             3             1   \n",
            "\n",
            "   satisfaction with workload  satisfied  compensation   \\\n",
            "0                           4                         3   \n",
            "1                           2                         2   \n",
            "2                           1                         2   \n",
            "3                           2                         2   \n",
            "4                           2                         3   \n",
            "\n",
            "    good relationship with peers    satisfied with career and  opportunity  \\\n",
            "0                               4                                        4   \n",
            "1                               2                                        2   \n",
            "2                               2                                        2   \n",
            "3                               2                                        2   \n",
            "4                               2                                        2   \n",
            "\n",
            "   satisfied with job profession    monthly average expenditure  \\\n",
            "0                               4                             2   \n",
            "1                               2                             4   \n",
            "2                               2                             3   \n",
            "3                               2                             4   \n",
            "4                               3                             3   \n",
            "\n",
            "    satisfied with  work-life balance   work is meaningful   family supports   \\\n",
            "0                                   4                     4                 4   \n",
            "1                                   2                     2                 2   \n",
            "2                                   2                     2                 2   \n",
            "3                                   2                     2                 2   \n",
            "4                                   2                     3                 2   \n",
            "\n",
            "    mentally well and do not have anxiety   TOI (turnover intention)  \n",
            "0                                        4                         0  \n",
            "1                                        2                         1  \n",
            "2                                        2                         1  \n",
            "3                                        2                         0  \n",
            "4                                        2                         1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('new neumerical.csv')  # Replace with your file path\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = data[\"TOI (turnover intention)\"]\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Display the top features\n",
        "print(importance_scores.head(17))  # Top 5 features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auj8nRi-PbNm",
        "outputId": "6ed2590f-bf2d-4536-f6c4-e27c748180bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance\n",
            "6               remainder__satisfaction with workload    0.051887\n",
            "9   remainder__ satisfied with career and  opportu...    0.021698\n",
            "7                 remainder__satisfied  compensation     0.019811\n",
            "11            remainder__ monthly average expenditure    0.009434\n",
            "15  remainder__ mentally well and do not have anxi...    0.006604\n",
            "14                        remainder__family supports     0.002830\n",
            "2                              remainder__Work tenure    0.001887\n",
            "8           remainder__ good relationship with peers     0.001887\n",
            "10          remainder__satisfied with job profession     0.000943\n",
            "4                             remainder__Job position    0.000000\n",
            "12      remainder__ satisfied with  work-life balance    0.000000\n",
            "0                                      remainder__Age   -0.000943\n",
            "13                    remainder__ work is meaningful    -0.000943\n",
            "3                                remainder__Education   -0.001887\n",
            "5                             remainder__working hour   -0.001887\n",
            "1                                   remainder__Gender   -0.008491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('new neumerical.csv')  # Replace with your file path\n",
        "\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"TOI (turnover intention)\"])  # Replace \"target_column\" with your target variable\n",
        "y = data[\"TOI (turnover intention)\"]\n",
        "\n",
        "\n",
        "# Identify categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Apply one-hot encoding to categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "# Transform features\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Map feature importance back to feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "importance_scores = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": perm_importance.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Handle negative importance values by explanation\n",
        "importance_scores[\"Remark\"] = importance_scores[\"Importance\"].apply(\n",
        "    lambda x: \"May introduce noise\" if x < 0 else \"Useful\"\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "print(importance_scores)\n",
        "\n",
        "# Optionally, save the feature importance rankings to a CSV\n",
        "importance_scores.to_csv('feature_importance_ranking.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9MXSrXKVHQX",
        "outputId": "603896fb-9470-48d9-c6b4-ec6c1d2db249"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Feature  Importance  \\\n",
            "6               remainder__satisfaction with workload    0.051887   \n",
            "9   remainder__ satisfied with career and  opportu...    0.021698   \n",
            "7                 remainder__satisfied  compensation     0.019811   \n",
            "11            remainder__ monthly average expenditure    0.009434   \n",
            "15  remainder__ mentally well and do not have anxi...    0.006604   \n",
            "14                        remainder__family supports     0.002830   \n",
            "2                              remainder__Work tenure    0.001887   \n",
            "8           remainder__ good relationship with peers     0.001887   \n",
            "10          remainder__satisfied with job profession     0.000943   \n",
            "4                             remainder__Job position    0.000000   \n",
            "12      remainder__ satisfied with  work-life balance    0.000000   \n",
            "0                                      remainder__Age   -0.000943   \n",
            "13                    remainder__ work is meaningful    -0.000943   \n",
            "3                                remainder__Education   -0.001887   \n",
            "5                             remainder__working hour   -0.001887   \n",
            "1                                   remainder__Gender   -0.008491   \n",
            "\n",
            "                 Remark  \n",
            "6                Useful  \n",
            "9                Useful  \n",
            "7                Useful  \n",
            "11               Useful  \n",
            "15               Useful  \n",
            "14               Useful  \n",
            "2                Useful  \n",
            "8                Useful  \n",
            "10               Useful  \n",
            "4                Useful  \n",
            "12               Useful  \n",
            "0   May introduce noise  \n",
            "13  May introduce noise  \n",
            "3   May introduce noise  \n",
            "5   May introduce noise  \n",
            "1   May introduce noise  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"mentally well and do not have anxity\",\n",
        "    \"family supports\",\n",
        "    \"satisfied with career and  opportunity\",\n",
        "    \"Work tenure\",\n",
        "    \"monthly average expenditure \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "# Step 1: Normalize the Decision Matrix\n",
        "def normalize(X):\n",
        "    norm_factors = np.sqrt(np.sum(X**2, axis=0))\n",
        "    return X / norm_factors\n",
        "\n",
        "normalized_data = normalize(data)\n",
        "\n",
        "# Step 2: Calculate Proportions (p_ij)\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Step 3: Calculate Entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Step 4: Calculate Weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "\n",
        "# Step 5: Combine Weights\n",
        "alpha, beta = 0.5, 0.5  # Equal weight contributions\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Step 6: Compile Results into a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Display the results\n",
        "print(\"Entropy Weight Results:\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqpL7gLSJW8b",
        "outputId": "930fed5c-ea1e-4f02-eb2a-a3abe97c3e78"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy Weight Results:\n",
            "                                        Entropy (e_j)  Weight w_e1(j)  \\\n",
            "satisfied  compensation                      0.957687        0.018597   \n",
            "satisfaction with workload                   0.981912        0.007950   \n",
            "satisfied with job profession                0.982542        0.007673   \n",
            "mentally well and do not have anxity         0.469992        0.232939   \n",
            "family supports                              0.826744        0.076146   \n",
            "satisfied with career and  opportunity       0.301030        0.307198   \n",
            "Work tenure                                  0.939794        0.026461   \n",
            "monthly average expenditure                  0.301030        0.307198   \n",
            "good relationship with peers                 0.985212        0.006499   \n",
            "Job position                                 0.978749        0.009340   \n",
            "\n",
            "                                        Weight w_e2(j)  Final Weight w_e(j)  \n",
            "satisfied  compensation                       0.064606             0.041601  \n",
            "satisfaction with workload                    0.063012             0.035481  \n",
            "satisfied with job profession                 0.062972             0.035322  \n",
            "mentally well and do not have anxity          0.131646             0.182293  \n",
            "family supports                               0.074839             0.075492  \n",
            "satisfied with career and  opportunity        0.205536             0.256367  \n",
            "Work tenure                                   0.065836             0.046148  \n",
            "monthly average expenditure                   0.205536             0.256367  \n",
            "good relationship with peers                  0.062801             0.034650  \n",
            "Job position                                  0.063216             0.036278  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQfl992oX2hw",
        "outputId": "e18f72d5-8ef2-40b4-8777-35cdba288659"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/244.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "# Dataset from the user\n",
        "data = np.array([\n",
        "    [2, 3, 1, 3, 0, 1, 4, 1, 1, 8],\n",
        "    [3, 2, 2, 3, 1, 0, 1, 0, 2, 5],\n",
        "    [3, 2, 2, 0, 3, 0, 2, 0, 2, 6],\n",
        "    [3, 2, 2, 2, 3, 0, 2, 0, 2, 6],\n",
        "    [1, 2, 2, 0, 0, 0, 1, 0, 3, 4],\n",
        "    [1, 3, 2, 0, 3, 0, 4, 0, 2, 5],\n",
        "    [2, 2, 1, 0, 0, 0, 1, 0, 2, 7],\n",
        "    [1, 3, 2, 0, 3, 1, 2, 1, 3, 2],\n",
        "    [2, 3, 1, 0, 3, 0, 1, 0, 2, 7],\n",
        "    [1, 1, 2, 0, 3, 0, 2, 0, 2, 5]\n",
        "])\n",
        "\n",
        "# Column names\n",
        "columns = [\n",
        "    \"satisfied  compensation\",\n",
        "    \"satisfaction with workload\",\n",
        "    \"satisfied with job profession\",\n",
        "    \"mentally well and do not have anxity\",\n",
        "    \"family supports\",\n",
        "    \"satisfied with career and  opportunity\",\n",
        "    \"Work tenure\",\n",
        "    \"monthly average expenditure \",\n",
        "    \"good relationship with peers\",\n",
        "    \"Job position\"\n",
        "]\n",
        "\n",
        "# Normalize the data\n",
        "normalized_data = data / np.sqrt(np.sum(data**2, axis=0))\n",
        "\n",
        "# Calculate proportions\n",
        "p_ij = normalized_data / np.sum(normalized_data, axis=0)\n",
        "\n",
        "# Calculate entropy (e_j)\n",
        "def calculate_entropy(p_ij, n):\n",
        "    p_ij = np.where(p_ij == 0, 1e-10, p_ij)  # Avoid log(0)\n",
        "    entropy = -np.sum(p_ij * np.log(p_ij), axis=0) / np.log(n)\n",
        "    return entropy\n",
        "\n",
        "n, m = data.shape  # n: rows, m: columns\n",
        "e_j = calculate_entropy(p_ij, n)\n",
        "\n",
        "# Calculate weights\n",
        "w_e1 = (1 - e_j) / np.sum(1 - e_j)\n",
        "w_e2 = (1 / e_j) / np.sum(1 / e_j)\n",
        "alpha, beta = 0.5, 0.5\n",
        "w_e = alpha * w_e1 + beta * w_e2\n",
        "\n",
        "# Recreate the results DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Entropy (e_j)\": e_j,\n",
        "    \"Weight w_e1(j)\": w_e1,\n",
        "    \"Weight w_e2(j)\": w_e2,\n",
        "    \"Final Weight w_e(j)\": w_e\n",
        "}, index=columns)\n",
        "\n",
        "# Recreate the document\n",
        "doc = Document()\n",
        "doc.add_heading(\"Entropy Weight Results\", level=1)\n",
        "\n",
        "# Add a table with the results\n",
        "table = doc.add_table(rows=1, cols=5)\n",
        "table.style = 'Table Grid'\n",
        "\n",
        "# Add headers to the table\n",
        "headers = [\"Column Name\", \"Entropy (e_j)\", \"Weight w_e1(j)\", \"Weight w_e2(j)\", \"Final Weight w_e(j)\"]\n",
        "header_cells = table.rows[0].cells\n",
        "for i, header in enumerate(headers):\n",
        "    header_cells[i].text = header\n",
        "\n",
        "# Add rows to the table\n",
        "for idx, row in results.iterrows():\n",
        "    row_cells = table.add_row().cells\n",
        "    row_cells[0].text = idx  # Column Name\n",
        "    row_cells[1].text = f\"{row['Entropy (e_j)']:.6f}\"\n",
        "    row_cells[2].text = f\"{row['Weight w_e1(j)']:.6f}\"\n",
        "    row_cells[3].text = f\"{row['Weight w_e2(j)']:.6f}\"\n",
        "    row_cells[4].text = f\"{row['Final Weight w_e(j)']:.6f}\"\n",
        "\n",
        "# Save the document\n",
        "file_path = \"Entropy_Weight_Results.docx\"\n",
        "doc.save(file_path)\n",
        "\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eKmYBwEwLO2Y",
        "outputId": "7586b5a0-d3e9-4768-c4ce-0458fb9c4590"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Entropy_Weight_Results.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'new neumerical.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        " # Drop irrelevant/low-importance columns\n",
        "columns_to_drop = [\n",
        "    ' satisfied with  work-life balance',\n",
        "    'Age',\n",
        "    ' work is meaningful ',\n",
        "    'Education',\n",
        "    'working hour',\n",
        "    'Gender'\n",
        "]\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "# Extract the decision matrix (assuming all numerical columns after the first one)\n",
        "decision_matrix = data.iloc[:, 1:].to_numpy()  # Adjust index based on column arrangement\n",
        "\n",
        "# Updated weights for TOPSIS based on the new columns and final weights\n",
        "weights = np.array([\n",
        "    0.041601,  # satisfied compensation\n",
        "    0.035481,  # satisfaction with workload\n",
        "    0.035322,  # satisfied with job profession\n",
        "    0.182293,  # mentally well and do not have anxity\n",
        "    0.075492,  # family supports\n",
        "    0.256367,  # satisfied with career and  opportunity\n",
        "    0.046148,  # Work tenure\n",
        "    0.256367,  # monthly average expenditure\n",
        "    0.034650,  # good relationship with peers\n",
        "    0.036278   # Job position\n",
        "])\n",
        "# Define the TOPSIS method\n",
        "def topsis(decision_matrix, weights):\n",
        "    # Normalize the decision matrix\n",
        "    norm_factors = np.sqrt(np.sum(decision_matrix**2, axis=0))\n",
        "    normalized_matrix = decision_matrix / norm_factors\n",
        "\n",
        "    # Apply weights\n",
        "    weighted_matrix = normalized_matrix * weights\n",
        "\n",
        "    # Determine positive and negative ideals\n",
        "    positive_ideal = np.max(weighted_matrix, axis=0)\n",
        "    negative_ideal = np.min(weighted_matrix, axis=0)\n",
        "\n",
        "    # Calculate distances to the positive and negative ideals\n",
        "    distance_positive = np.sqrt(np.sum((weighted_matrix - positive_ideal) ** 2, axis=1))\n",
        "    distance_negative = np.sqrt(np.sum((weighted_matrix - negative_ideal) ** 2, axis=1))\n",
        "\n",
        "    # Calculate closeness coefficients\n",
        "    closeness_coefficients = distance_negative / (distance_positive + distance_negative)\n",
        "    return closeness_coefficients\n",
        "\n",
        "# Compute TOPSIS closeness coefficients\n",
        "closeness_coefficients = topsis(decision_matrix, weights)\n",
        "\n",
        "# Categorize employees based on closeness coefficients\n",
        "maxR = closeness_coefficients.max()\n",
        "minR = closeness_coefficients.min()\n",
        "Nclass = 3  # Number of categories\n",
        "D = (maxR - minR) / Nclass\n",
        "\n",
        "distressed_range = (minR, minR + D)\n",
        "behavioral_range = (minR + D, minR + 2 * D)\n",
        "enthusiastic_range = (minR + 2 * D, maxR)\n",
        "\n",
        "# Assign categories based on the ranges\n",
        "categories_named = []\n",
        "for cc in closeness_coefficients:\n",
        "    if distressed_range[0] <= cc <= distressed_range[1]:\n",
        "        categories_named.append(\"Distressed\")\n",
        "    elif behavioral_range[0] < cc <= behavioral_range[1]:\n",
        "        categories_named.append(\"Behavioral\")\n",
        "    elif enthusiastic_range[0] < cc <= enthusiastic_range[1]:\n",
        "        categories_named.append(\"Enthusiastic\")\n",
        "\n",
        "# Add results to the original dataset\n",
        "data['Closeness Coefficient'] = closeness_coefficients\n",
        "data['Category'] = categories_named\n",
        "\n",
        "# Save the updated dataset\n",
        "output_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Updated dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtmaWOQMBwd",
        "outputId": "524abd96-e10c-4f40-ced8-28f9b760e1ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset saved to updated_employee_categorization_new_weights.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded dataset with categorization\n",
        "file_path = 'updated_employee_categorization_new_weights.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Split the data into three categories\n",
        "distressed_data = data[data['Category'] == 'Distressed']\n",
        "behavioral_data = data[data['Category'] == 'Behavioral']\n",
        "enthusiastic_data = data[data['Category'] == 'Enthusiastic']\n",
        "\n",
        "# Save each category into separate files\n",
        "distressed_file = 'distressed_employees.xlsx'\n",
        "behavioral_file = 'behavioral_employees.xlsx'\n",
        "enthusiastic_file = 'enthusiastic_employees.xlsx'\n",
        "\n",
        "distressed_data.to_excel(distressed_file, index=False)\n",
        "behavioral_data.to_excel(behavioral_file, index=False)\n",
        "enthusiastic_data.to_excel(enthusiastic_file, index=False)\n",
        "\n",
        "distressed_file, behavioral_file, enthusiastic_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MujnfCmEe9Dc",
        "outputId": "27a6d249-159b-4d0c-8f15-ee21b9ceb4bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('distressed_employees.xlsx',\n",
              " 'behavioral_employees.xlsx',\n",
              " 'enthusiastic_employees.xlsx')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'distressed_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\")\n",
        "print(data['TOI (turnover intention)'].value_counts())\n",
        "\n",
        "# Target sample sizes\n",
        "target_class_0 = 2764\n",
        "target_class_1 = 2763\n",
        "\n",
        "# Oversample Class 0\n",
        "if len(class_0) > 0:\n",
        "    class_0_balanced = resample(\n",
        "        class_0,\n",
        "        replace=True,\n",
        "        n_samples=target_class_0,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 0 has zero samples. Skipping resampling.\")\n",
        "    class_0_balanced = pd.DataFrame()\n",
        "\n",
        "# Oversample Class 1\n",
        "if len(class_1) > 0:\n",
        "    class_1_balanced = resample(\n",
        "        class_1,\n",
        "        replace=True,\n",
        "        n_samples=target_class_1,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(\"Class 1 has zero samples. Skipping resampling.\")\n",
        "    class_1_balanced = pd.DataFrame()\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "output_path = 'balanced_dataset-distressed.csv'\n",
        "balanced_data.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(\"Class Distribution After Balancing:\")\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n",
        "print(f\"Balanced dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kxP-IlOcHAi",
        "outputId": "eb586bbf-51da-49eb-e499-88dc2b9952e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution Before Balancing:\n",
            "TOI (turnover intention)\n",
            "1    121\n",
            "0      9\n",
            "Name: count, dtype: int64\n",
            "Class Distribution After Balancing:\n",
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to balanced_dataset-distressed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-distressed.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,           # Fewer trees\n",
        "    max_depth=3,               # Limit depth\n",
        "    min_samples_split=10,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDlBVseOoIi",
        "outputId": "dbec7302-2ecb-4f81-ba80-9f268379fa5e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[455  98]\n",
            " [ 27 525]]\n",
            "\n",
            "Accuracy: 0.8869\n",
            "Precision: 0.8427\n",
            "Recall: 0.9511\n",
            "F1-Score: 0.8936\n",
            "MCC: 0.7802\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.82      0.88       553\n",
            "           1       0.84      0.95      0.89       552\n",
            "\n",
            "    accuracy                           0.89      1105\n",
            "   macro avg       0.89      0.89      0.89      1105\n",
            "weighted avg       0.89      0.89      0.89      1105\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'behavioral_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-behavioral.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYuuXnOegLt",
        "outputId": "10880689-a50c-4982-fa46-3f97f6a4ea83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-behavioral.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGuDNlWeO90A",
        "outputId": "bf2ad06c-7626-4f37-afe7-5575d4812997"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[534  17]\n",
            " [ 54 497]]\n",
            "\n",
            "Accuracy: 0.9356\n",
            "Precision: 0.9669\n",
            "Recall: 0.9020\n",
            "F1-Score: 0.9333\n",
            "MCC: 0.8731\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       551\n",
            "           1       0.97      0.90      0.93       551\n",
            "\n",
            "    accuracy                           0.94      1102\n",
            "   macro avg       0.94      0.94      0.94      1102\n",
            "weighted avg       0.94      0.94      0.94      1102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'enthusiastic_employees.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI (turnover intention)'] == 0]\n",
        "class_1 = data[data['TOI (turnover intention)'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-enthusiastic.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI (turnover intention)'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1P55t_xPtX7",
        "outputId": "093e08f9-9575-4f58-8088-471859977a5c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI (turnover intention)\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the balanced dataset\n",
        "file_path = 'balanced_dataset-enthusiastic.csv'  # Adjust file name if necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=['TOI (turnover intention)', ' Closeness Coefficient', 'Category'], errors='ignore')\n",
        "y = data['TOI (turnover intention)']\n",
        "\n",
        "# Add noise to the features\n",
        "X_noisy = X + np.random.normal(0, 0.01, X.shape)  # Higher noise level\n",
        "\n",
        "# Randomly flip some target labels to introduce noise\n",
        "flip_fraction = 0.01  # Flip 10% of labels\n",
        "indices = np.random.choice(y.index, size=int(len(y) * flip_fraction), replace=False)\n",
        "y_noisy = y.copy()\n",
        "y_noisy.loc[indices] = 1 - y.loc[indices]  # Flip labels\n",
        "\n",
        "# Resample the dataset (optional)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_noisy, y_noisy)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,           # Fewer trees\n",
        "    max_depth=5,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ssS5D_PthG",
        "outputId": "7ccebfcb-7130-493d-f9e6-5e439c31084c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[528  24]\n",
            " [ 70 482]]\n",
            "\n",
            "Accuracy: 0.9149\n",
            "Precision: 0.9526\n",
            "Recall: 0.8732\n",
            "F1-Score: 0.9112\n",
            "MCC: 0.8326\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       552\n",
            "           1       0.95      0.87      0.91       552\n",
            "\n",
            "    accuracy                           0.91      1104\n",
            "   macro avg       0.92      0.91      0.91      1104\n",
            "weighted avg       0.92      0.91      0.91      1104\n",
            "\n"
          ]
        }
      ]
    }
  ]
}