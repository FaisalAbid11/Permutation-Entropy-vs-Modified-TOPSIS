{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "57kkdclTTw4X",
        "outputId": "bcfc1524-45b0-4745-e0d7-a48b94f0df6c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6419cea-33a0-4dfd-994a-0c907315af64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6419cea-33a0-4dfd-994a-0c907315af64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modified ahp-topsis 20.csv to modified ahp-topsis 20.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'modified ahp-topsis 20.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-modified-20.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6RtCk1T05i",
        "outputId": "62cd48b4-0c03-47f5-eb74-a989a6249ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset-modified-20.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=10,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=20,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBLtKPTXVBfz",
        "outputId": "e0410e65-e7a2-4b6e-d7e7-795bf204f7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[553   0]\n",
            " [ 13 540]]\n",
            "\n",
            "Accuracy: 0.9882\n",
            "Precision: 1.0000\n",
            "Recall: 0.9765\n",
            "F1-Score: 0.9881\n",
            "MCC: 0.9768\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       553\n",
            "           1       1.00      0.98      0.99       553\n",
            "\n",
            "    accuracy                           0.99      1106\n",
            "   macro avg       0.99      0.99      0.99      1106\n",
            "weighted avg       0.99      0.99      0.99      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "7                             working hour    0.254539\n",
            "9                      work is meaningful     0.145825\n",
            "6   satisfied with career and  opportunity    0.128667\n",
            "2   mentally well and do not have anxiety     0.108528\n",
            "5                         family supports     0.094616\n",
            "4                 satisfied  compensation     0.081118\n",
            "8            good relationship with peers     0.075846\n",
            "1        satisfied with  work-life balance    0.057259\n",
            "3           satisfied with job profession     0.032515\n",
            "0               satisfaction with workload    0.021087\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "UpVReQLrT1HM",
        "outputId": "adfb503c-61a6-44cb-d0ad-43e5f0a2f9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd68e903-fd71-44a1-8e1a-78d87f1ba990\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd68e903-fd71-44a1-8e1a-78d87f1ba990\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modified ahp-topsis 30.csv to modified ahp-topsis 30.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'modified ahp-topsis 30.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset_modified-30.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGh5kljvUY9M",
        "outputId": "21ba9455-3448-4e2e-af60-a94a85ab2c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=10,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=20,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")"
      ],
      "metadata": {
        "id": "l-2xPFpMS7Iu",
        "outputId": "a8cf7aed-f45b-4dbc-968b-ebfa3895df6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[535  18]\n",
            " [ 30 523]]\n",
            "\n",
            "Accuracy: 0.9566\n",
            "Precision: 0.9667\n",
            "Recall: 0.9458\n",
            "F1-Score: 0.9561\n",
            "MCC: 0.9134\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       553\n",
            "           1       0.97      0.95      0.96       553\n",
            "\n",
            "    accuracy                           0.96      1106\n",
            "   macro avg       0.96      0.96      0.96      1106\n",
            "weighted avg       0.96      0.96      0.96      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "1        satisfied with  work-life balance    0.169405\n",
            "7                             working hour    0.113097\n",
            "4                 satisfied  compensation     0.111073\n",
            "2   mentally well and do not have anxiety     0.109101\n",
            "0               satisfaction with workload    0.101017\n",
            "6   satisfied with career and  opportunity    0.100761\n",
            "8            good relationship with peers     0.094142\n",
            "5                         family supports     0.077942\n",
            "9                      work is meaningful     0.064205\n",
            "3           satisfied with job profession     0.059256\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=15,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=20,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Xcul7qVr0q",
        "outputId": "4e06f685-daf5-49a6-e0f6-c1f0e4134460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[535  18]\n",
            " [ 30 523]]\n",
            "\n",
            "Accuracy: 0.9566\n",
            "Precision: 0.9667\n",
            "Recall: 0.9458\n",
            "F1-Score: 0.9561\n",
            "MCC: 0.9134\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       553\n",
            "           1       0.97      0.95      0.96       553\n",
            "\n",
            "    accuracy                           0.96      1106\n",
            "   macro avg       0.96      0.96      0.96      1106\n",
            "weighted avg       0.96      0.96      0.96      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "1        satisfied with  work-life balance    0.173656\n",
            "7                             working hour    0.118094\n",
            "4                 satisfied  compensation     0.109963\n",
            "2   mentally well and do not have anxiety     0.108003\n",
            "0               satisfaction with workload    0.102346\n",
            "6   satisfied with career and  opportunity    0.100396\n",
            "8            good relationship with peers     0.095357\n",
            "5                         family supports     0.077731\n",
            "9                      work is meaningful     0.060371\n",
            "3           satisfied with job profession     0.054085\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature selection using Random Forest\n",
        "print(\"Performing feature selection...\")\n",
        "sfm = SelectFromModel(RandomForestClassifier(random_state=42), threshold=0.01)\n",
        "sfm.fit(X_train, y_train)\n",
        "X_train_selected = sfm.transform(X_train)\n",
        "X_test_selected = sfm.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [15, 20, 25],\n",
        "    'min_samples_split': [10, 20],\n",
        "    'min_samples_leaf': [5, 10],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "\n",
        "# Use the best model from GridSearchCV\n",
        "rf = grid_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train the model on the selected features\n",
        "rf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns[sfm.get_support()],\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0t-KINALQI-",
        "outputId": "839e627b-77ca-4be8-f564-c843220fee26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing feature selection...\n",
            "Performing hyperparameter tuning...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Best Hyperparameters: {'class_weight': None, 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "\n",
            "Confusion Matrix:\n",
            "[[482  71]\n",
            " [  6 547]]\n",
            "\n",
            "Accuracy: 0.9304\n",
            "F1-Score: 0.9342\n",
            "MCC: 0.8668\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93       553\n",
            "           1       0.89      0.99      0.93       553\n",
            "\n",
            "    accuracy                           0.93      1106\n",
            "   macro avg       0.94      0.93      0.93      1106\n",
            "weighted avg       0.94      0.93      0.93      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "1        satisfied with  work-life balance    0.152376\n",
            "4                 satisfied  compensation     0.145617\n",
            "8            good relationship with peers     0.144435\n",
            "5                         family supports     0.098954\n",
            "2   mentally well and do not have anxiety     0.083651\n",
            "7                             working hour    0.082932\n",
            "6   satisfied with career and  opportunity    0.079600\n",
            "9                      work is meaningful     0.077252\n",
            "3           satisfied with job profession     0.075438\n",
            "0               satisfaction with workload    0.059745\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "      # Number of trees in the forest\n",
        "    random_state=42          # Set random state for reproducibility\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')  # Adjust 'binary' if it's a multi-class problem\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVsHYTYEOWZU",
        "outputId": "1d40a9b0-d43b-4d89-8edb-8403cc1c78dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[482  71]\n",
            " [  6 547]]\n",
            "\n",
            "Accuracy: 0.9304\n",
            "F1-Score: 0.9342\n",
            "MCC: 0.8668\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93       553\n",
            "           1       0.89      0.99      0.93       553\n",
            "\n",
            "    accuracy                           0.93      1106\n",
            "   macro avg       0.94      0.93      0.93      1106\n",
            "weighted avg       0.94      0.93      0.93      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "8            good relationship with peers     0.155785\n",
            "1        satisfied with  work-life balance    0.149657\n",
            "4                 satisfied  compensation     0.140938\n",
            "5                         family supports     0.099787\n",
            "2   mentally well and do not have anxiety     0.084238\n",
            "7                             working hour    0.082310\n",
            "6   satisfied with career and  opportunity    0.077825\n",
            "3           satisfied with job profession     0.074523\n",
            "9                      work is meaningful     0.073290\n",
            "0               satisfaction with workload    0.061647\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Random Forest with optimized parameters\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,         # Increased number of trees\n",
        "    max_depth=20,             # Limit depth to prevent overfitting\n",
        "    min_samples_split=10,     # Larger minimum samples to split\n",
        "    min_samples_leaf=5,       # Larger minimum samples in leaves\n",
        "    class_weight='balanced',  # Handle imbalanced dataset\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')  # Adjust 'binary' for multi-class problems\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Cross-validation\n",
        "scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"\\nCross-Validation Accuracy: {scores.mean():.4f}  {scores.std():.4f}\")\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJFHz1LOPvj1",
        "outputId": "b9c540eb-46d3-474c-e6af-eb4c232cfd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[482  71]\n",
            " [  6 547]]\n",
            "\n",
            "Accuracy: 0.9304\n",
            "F1-Score: 0.9342\n",
            "MCC: 0.8668\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93       553\n",
            "           1       0.89      0.99      0.93       553\n",
            "\n",
            "    accuracy                           0.93      1106\n",
            "   macro avg       0.94      0.93      0.93      1106\n",
            "weighted avg       0.94      0.93      0.93      1106\n",
            "\n",
            "\n",
            "Cross-Validation Accuracy: 0.9164  0.0049\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "1        satisfied with  work-life balance    0.153878\n",
            "8            good relationship with peers     0.153068\n",
            "4                 satisfied  compensation     0.139844\n",
            "5                         family supports     0.100896\n",
            "2   mentally well and do not have anxiety     0.085475\n",
            "7                             working hour    0.083643\n",
            "6   satisfied with career and  opportunity    0.073845\n",
            "3           satisfied with job profession     0.072878\n",
            "9                      work is meaningful     0.071817\n",
            "0               satisfaction with workload    0.064655\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "Z7peanzxVFuf",
        "outputId": "a4676847-a20a-480c-aced-641d24b30cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecf7653d-a8f3-49fd-b0cd-473c497441ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecf7653d-a8f3-49fd-b0cd-473c497441ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modified 50 ahp.csv to modified 50 ahp.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'modified 50 ahp.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-modified-50.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8to2AWDVF79",
        "outputId": "c3b6355b-bc3d-468c-b34e-9042068a9d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset-modified-50.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=20,               # Limit depth\n",
        "    min_samples_split=10,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX24ljglWJok",
        "outputId": "59e79875-06f2-4384-b283-e63ca5060067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[550   3]\n",
            " [ 19 534]]\n",
            "\n",
            "Accuracy: 0.9801\n",
            "Precision: 0.9944\n",
            "Recall: 0.9656\n",
            "F1-Score: 0.9798\n",
            "MCC: 0.9606\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       553\n",
            "           1       0.99      0.97      0.98       553\n",
            "\n",
            "    accuracy                           0.98      1106\n",
            "   macro avg       0.98      0.98      0.98      1106\n",
            "weighted avg       0.98      0.98      0.98      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "6   satisfied with career and  opportunity    0.253545\n",
            "5                         family supports     0.149625\n",
            "2   mentally well and do not have anxiety     0.124625\n",
            "0               satisfaction with workload    0.119344\n",
            "3           satisfied with job profession     0.112557\n",
            "4                 satisfied  compensation     0.087810\n",
            "1        satisfied with  work-life balance    0.051404\n",
            "8            good relationship with peers     0.038634\n",
            "7                             working hour    0.037941\n",
            "9                      work is meaningful     0.024515\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    }
  ]
}