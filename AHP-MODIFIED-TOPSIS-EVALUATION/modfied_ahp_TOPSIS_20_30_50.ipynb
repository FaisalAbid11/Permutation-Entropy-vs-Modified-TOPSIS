{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-MODIFIED-TOPSIS-EVALUATION/distressed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-modified-20.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6RtCk1T05i",
        "outputId": "ac6c660c-3fdc-415a-e161-2ee324e3a3d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset-modified-20.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=10,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=20,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBLtKPTXVBfz",
        "outputId": "3de58e60-59a4-45a0-8ae3-08c791455a89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[553   0]\n",
            " [ 13 540]]\n",
            "\n",
            "Accuracy: 0.9882\n",
            "Precision: 1.0000\n",
            "Recall: 0.9765\n",
            "F1-Score: 0.9881\n",
            "MCC: 0.9768\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       553\n",
            "           1       1.00      0.98      0.99       553\n",
            "\n",
            "    accuracy                           0.99      1106\n",
            "   macro avg       0.99      0.99      0.99      1106\n",
            "weighted avg       0.99      0.99      0.99      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "7                             working hour    0.254539\n",
            "9                      work is meaningful     0.145825\n",
            "6   satisfied with career and  opportunity    0.128667\n",
            "2   mentally well and do not have anxiety     0.108528\n",
            "5                         family supports     0.094616\n",
            "4                 satisfied  compensation     0.081118\n",
            "8            good relationship with peers     0.075846\n",
            "1        satisfied with  work-life balance    0.057259\n",
            "3           satisfied with job profession     0.032515\n",
            "0               satisfaction with workload    0.021087\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-MODIFIED-TOPSIS-EVALUATION/behavioral.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset_modified-30.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGh5kljvUY9M",
        "outputId": "b69aba27-fae0-4132-ef28-c47a00aff744"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset_modified-30.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=10,               # Limit depth\n",
        "    min_samples_split=20,      # Larger minimum samples to split\n",
        "    min_samples_leaf=20,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")"
      ],
      "metadata": {
        "id": "l-2xPFpMS7Iu",
        "outputId": "43890195-90da-4b80-e620-7dfc74b75fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[535  18]\n",
            " [ 30 523]]\n",
            "\n",
            "Accuracy: 0.9566\n",
            "Precision: 0.9667\n",
            "Recall: 0.9458\n",
            "F1-Score: 0.9561\n",
            "MCC: 0.9134\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       553\n",
            "           1       0.97      0.95      0.96       553\n",
            "\n",
            "    accuracy                           0.96      1106\n",
            "   macro avg       0.96      0.96      0.96      1106\n",
            "weighted avg       0.96      0.96      0.96      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "1        satisfied with  work-life balance    0.169405\n",
            "7                             working hour    0.113097\n",
            "4                 satisfied  compensation     0.111073\n",
            "2   mentally well and do not have anxiety     0.109101\n",
            "0               satisfaction with workload    0.101017\n",
            "6   satisfied with career and  opportunity    0.100761\n",
            "8            good relationship with peers     0.094142\n",
            "5                         family supports     0.077942\n",
            "9                      work is meaningful     0.064205\n",
            "3           satisfied with job profession     0.059256\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/FaisalAbid11/Permutation-Entropy-vs-Modified-TOPSIS/refs/heads/main/AHP-MODIFIED-TOPSIS-EVALUATION/enthusiastic.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "class_0 = data[data['TOI'] == 0]\n",
        "class_1 = data[data['TOI'] == 1]\n",
        "\n",
        "# Oversample both classes to the target count (2764 for Class 0 and 2763 for Class 1)\n",
        "class_0_balanced = resample(class_0,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2764,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "class_1_balanced = resample(class_1,\n",
        "                            replace=True,     # Sample with replacement\n",
        "                            n_samples=2763,   # Target number of samples\n",
        "                            random_state=42)  # Reproducibility\n",
        "\n",
        "# Combine the two classes to form the balanced dataset\n",
        "balanced_data = pd.concat([class_0_balanced, class_1_balanced])\n",
        "\n",
        "# Shuffle the dataset to mix the classes\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a CSV file (optional)\n",
        "balanced_data.to_csv('balanced_dataset-modified-50.csv', index=False)\n",
        "\n",
        "# Display the new class distribution\n",
        "print(balanced_data['TOI'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8to2AWDVF79",
        "outputId": "02626f6d-ac90-4c53-a6be-86b13618c2b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOI\n",
            "0    2764\n",
            "1    2763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "## Load the dataset\n",
        "file_path = 'balanced_dataset-modified-50.csv'  # Update with the correct path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['TOI'])  # Features\n",
        "y = data['TOI']  # Target\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Random Forest Classifier with reduced complexity\n",
        "rf = RandomForestClassifier(\n",
        "   n_estimators=100,           # Fewer trees\n",
        "    max_depth=20,               # Limit depth\n",
        "    min_samples_split=10,      # Larger minimum samples to split\n",
        "    min_samples_leaf=10,       # Larger leaf size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the training set\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Save feature importance to a CSV file\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX24ljglWJok",
        "outputId": "beaf66f2-572d-4777-b742-393adb8e6844"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[553   0]\n",
            " [ 18 535]]\n",
            "\n",
            "Accuracy: 0.9837\n",
            "Precision: 1.0000\n",
            "Recall: 0.9675\n",
            "F1-Score: 0.9835\n",
            "MCC: 0.9680\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98       553\n",
            "           1       1.00      0.97      0.98       553\n",
            "\n",
            "    accuracy                           0.98      1106\n",
            "   macro avg       0.98      0.98      0.98      1106\n",
            "weighted avg       0.98      0.98      0.98      1106\n",
            "\n",
            "\n",
            "Feature Importances:\n",
            "                                   Feature  Importance\n",
            "0               satisfaction with workload    0.289388\n",
            "4                 satisfied  compensation     0.141941\n",
            "1        satisfied with  work-life balance    0.132315\n",
            "2   mentally well and do not have anxiety     0.125013\n",
            "3           satisfied with job profession     0.072487\n",
            "6   satisfied with career and  opportunity    0.061776\n",
            "7                             working hour    0.055250\n",
            "8            good relationship with peers     0.050855\n",
            "5                         family supports     0.036632\n",
            "9                      work is meaningful     0.034343\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    }
  ]
}